# Week 2 Data
## Ethics of Data Collection :: What is really is Fair Use?

- What is ethical data collection?  
- What is Fair Use?

From Wikipedia 

Fair use is a doctrine in United States law that permits limited use of copyrighted material without having to first acquire permission from the copyright holder. Fair use is one of the limitations to copyright intended to balance the interests of copyright holders with the public interest in the wider distribution and use of creative works by allowing as a defense to copyright infringement claims certain limited uses that might otherwise be considered infringement. The US "fair use doctrine" is generally broader than the "fair dealing" rights known in most countries that inherited English Common Law. The fair use right is a general exception, that applies to all different kinds of uses with all types of works. In the U.S., fair use right/exception is based on a flexible proportionality test, that examines the purpose of the use, the amount used, and the impact on the market of the original work.

The doctrine of "fair use" originated in common law during the 18th and 19th centuries as a way of preventing copyright law from being too rigidly applied and "stifling the very creativity which copyright law is designed to foster." Though originally a common law doctrine, it was enshrined in statutory law when the U.S. Congress passed the Copyright Act of 1976. The U.S. Supreme Court has issued several major decisions clarifying and reaffirming the fair use doctrine since the 1980s, the most recent being in the 2021 decision Google LLC v. Oracle America, Inc.


### Google LLC v. Oracle America, Inc

From Wikipedia 

Google LLC v. Oracle America, Inc., 593 U.S. ___ (2021), was a U.S. Supreme Court decision related to the nature of computer code and copyright law. The dispute centered on the use of parts of the Java programming language's application programming interfaces (APIs) and about 11,000 lines of source code, which are owned by Oracle (through subsidiary, Oracle America, Inc., originating from Sun Microsystems), within early versions of the Android operating system by Google. Google has since transitioned Android to a copyright-unburdened engine without the source code, and has admitted to using the APIs but claimed this was within fair use.

Oracle initiated the suit arguing that the APIs were copyrightable, seeking US$8.8 billion in damages from Google's sales and licensing of the earlier infringing versions of Android. While two District Court-level jury trials found in favor of Google, the Federal Circuit court reversed both decisions, holding that APIs are copyrightable and Google's use does not fall under fair use. Google successfully petitioned to the Supreme Court to hear the case in the 2019 term, focusing on the copyrightability of APIs and subsequent fair use; the case was delayed to the 2020 term due to the COVID-19 pandemic. In April 2021, the Supreme Court ruled in a 6–2 decision that Google's use of the Java APIs fell within the four factors of fair use, bypassing the question on the copyrightability of the APIs. The decision reversed the Federal Circuit ruling and remanded the case for further review.

The case has been of significant interest within the tech and software industries, as numerous computer programs and software libraries, particularly in open source, are developed by recreating the functionality of APIs from commercial or competing products to aid developers in interoperability between different systems or platforms.

```
The Court took a newly expansive view of transformativeness in the fair use analysis, 
recognizing the significance of Google’s “reimplementation” of the Java API in a new 
context and the value of the third-party creativity the Android platform enabled. 
Although this development may feed concerns that fair use impinges on the transformation 
central to the derivative works right held exclusively by copyright owners,9 the Court’s 
expansion of transformativeness in fair use accords with the constitutional goals of the Copyright Act.
```

#### New Reading [Harvard Law Review](./readings/135-Harv.-L.-Rev.-431.pdf)

### New York Times v. OpenAI

[NYT Page](https://www.nytco.com/press/lawsuit-documents-dec-2023/#:~:text=On%20Dec.%2027%2C%202023%2C%20The%20New%20York%20Times,and%20wrongly%20attribute%20false%20information%20to%20the%20Times.)

[New York Times v OpenAI Complaint](https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf)

```
Defendants’ unlawful use of The Times’s work to create artificial intelligence 
products that compete with it threatens The Times’s ability to provide that service. Defendants’ 
generative artificial intelligence (“GenAI”) tools rely on large-language models (“LLMs”) that 
were built by copying and using millions of The Times’s copyrighted news articles, in-depth 
investigations, opinion pieces, reviews, how-to guides, and more. While Defendants engaged in 
widescale copying from many sources, they gave Times content particular emphasis when building 
their LLMs—revealing a preference that recognizes the value of those works. Through Microsoft’s 
Bing Chat (recently rebranded as “Copilot”) and OpenAI’s ChatGPT, Defendants seek to free-ride 
on The Times’s massive investment in its journalism by using it to build substitutive products 
without permission or payment

Defendants have refused to recognize this protection. Powered by LLMs containing
copies of Times content, Defendants’ GenAI tools can generate output that recites Times content 
verbatim, closely summarizes it, and mimics its expressive style, as demonstrated by scores of 
examples. See Exhibit J. These tools also wrongly attribute false information to The Times. 
```

### Discussion Time

What do you think?
What is and isn't Fair Use?
What is and isn't 'sufficiently` changed or altered?

How are images produced from an initial noise source being multiplied by a transformation matrix of weights derived from millions of source images being noised and de-noised using those estimated weights not 'sufficiently' altering the image? 
What about Collages? Lamination? Remixes?

# Data Collection and Scraping Techniques

## Scraping as Art

#### Discussion [Scrapism A Manifesto](./readings/Scrapism-A-Manifesto_2.pdf)

Sam Lavigne (born 1981) is an artist and educator based in New York. His work deals with technology, data, surveillance, natural language processing, and automation. Lavigne describes his work as "online interventions that surface the frequently opaque political and economic conditions that shape computational technologies"

- [3 Degrees of Separation from the Military-Industrial-Prison-Data-Surveillance State](https://lav.io/projects/3-degrees-of-separation/)
- [New York Apartment](https://github.com/antiboredom/new-york-apartment)
- [Carbon Offset Scraper](https://github.com/antiboredom/carbon-offset-scraper)
- [Training Poses](https://lav.io/projects/training-poses/)
- [Database of ICE employee LinkedIn accounts](https://www.theverge.com/2018/6/19/17480912/github-ice-linkedin-scraping-employees)

### Other Scraping Examples
- [The Markup](https://github.com/the-markup/investigation-amazon-covid)

## Scraping Facts
- Every website is different
- The internet is in a constant state of change
- Your process will break

General structures repeat themselves, yet each website is unique and will need personal treatment if you want to extract the relevant information.  Websites constantly changes! Your scraper will no doubt stop working due to changes in the API or HTML.

### [Web Scraping with Beautiful Soup](./notebooks/0_Web_Scraping_With_Beautiful_Soap.ipynb)

### [Web Scraping with an API](./notebooks/0_Scraping_your_data.ipynb)

### Open(ish) Data Sources

- [NYC OPEN DATA](https://opendata.cityofnewyork.us/)
- [Getting started with Wikimedia APIs](https://api.wikimedia.org/wiki/Getting_started_with_Wikimedia_APIs)
- [ADS-B](https://www.adsbexchange.com/) 
  - ### PUT LINK TO KYLE'S DATA ON GOOGLE DRIVE

## Formatting: Writing the Proto-Prompts 

### Preparing Your Encoded dataset

One way of preparing your data is to pre-process it and encode it into the proto-prompt format.  This is one way the data is encoded into Natural Language that the transformer can understand.  It allows us to transform structured data into human readable text and processed through the Natural Language Process.  

[Encoding Your Data](./notebooks/1_Encoding_your_data.ipynb)

### Preparing Your Structured Data

Some researcher have experimented with training transformers with Structured Data. A team at Microsoft Research has released a suite of small language models (SLMs) called “Phi” that achieve remarkable performance on a variety of benchmarks. Their first model, the 1.3 billion parameter Phi-1, achieved state-of-the-art performance on Python coding.  They then extended their focus to common sense reasoning and language understanding and created a new 1.3 billion parameter model named Phi-1.5, with performance comparable to models 5x larger. They recently released Phi-2, a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation. [Link](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)

#### Phi-2

[Encoding Your Structured Data](./notebooks/1_Encoding_your_structure_data.ipynb)

## Evaluating your data: Training your Tokenizer

#### [BERT Tokenizer From Scratch](./notebooks/2_BERT_tokenizer_from_scratch.ipynb)
#### [Expand a Tokenizer](./notebooks/2_Expand_A_Tokenizer.ipynb)

### Identifying Bias in your Data

To identify bias in data, you can:
- Identify the sources of data collection and analyze them to gauge any possible bias.
  - Where is the data from?
  - How could it be biased?
- Review the data collection process to know the presence of any factor leading to bias in data.
  - What are their sources?
  - How was it collected?
  - What was the population size?
- Identify any special pattern in the data, indicating bias.
  - are there many duplicates?
  - how different is the data?
  - how unique is each sample?
- Check whether the protected groups that could be impacted by the AI system are well represented in the dataset.
  - Who made up the population?
  - How could people have been left out of the data?
  - What groups are over represented in the data?
- Compare the data quality of the protected group with that of the rest of the population.

# Pitch Your Books! 

# Homework
1. Write a script that builds a data set
   1. Find a dataset online and convert it to: 
      1. prompt template of text encoded data
      2. structured data like JSON strings
   2. Think about what problem you are trying to solve and what data you need to give it
   3. Format that data in a way you can parse from the output of an LLM
   4. Due Week 4

## Reading List
- [Harvard Law Review Google v Oracle-America](./readings/135-Harv.-L.-Rev.-431.pdf)
- [The impact of artificial intelligence on human society and bioethics](./readings/TCMJ-32-339.pdf)
- [This Program can give AI a sense of Ethics -Sometimes](./readings/program-give-ai-ethics-sometimes.pdf)
## Optional 
- [New York Times v OpenAI](https://nytco-assets.nytimes.com/2023/12/NYT_Complaint_Dec2023.pdf)