{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt6gMp3YD-n_"
      },
      "source": [
        "# Fast tokenizers' special powers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsDUOa6vG2gU"
      },
      "source": [
        "## Batch encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZdTWn8DD-oJ",
        "outputId": "2a2db692-b035-4979-92ca-35fb5c054b0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<class 'transformers.tokenization_utils_base.BatchEncoding'>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "encoding = tokenizer(example)\n",
        "print(type(encoding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvxNrhloD-oL",
        "outputId": "0f12da84-a848-49cd-e9c7-f30b62995e7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys_3VwMbD-oM",
        "outputId": "9c6b8990-a714-47e3-dc82-1954512093dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.is_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2TqYx-UD-oN",
        "outputId": "6e8d5ffe-c42a-4edd-a0ac-214cb24801b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',\n",
              " 'Brooklyn', '.', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfwLJnyqD-oP",
        "outputId": "05f84395-4219-497d-9514-6492a585a56a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoding.word_ids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIxSKt0GD-oQ",
        "outputId": "d50fff43-e205-438a-c171-2c4a0927f1f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sylvain"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start, end = encoding.word_to_chars(3)\n",
        "example[start:end]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6miqd_ZMW-H"
      },
      "source": [
        "## Inside the token-classification pipeline\n",
        "\n",
        "### Getting the base results with the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSmw-4JSD-oS",
        "outputId": "a72959b5-bc91-4a5b-b3be-5b7b84956d80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},\n",
              " {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},\n",
              " {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},\n",
              " {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},\n",
              " {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},\n",
              " {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},\n",
              " {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},\n",
              " {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_PaF3ZfD-oT",
        "outputId": "f6f11b4a-a7b7-4fd9-b875-4e2813444ec2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},\n",
              " {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},\n",
              " {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27sLwBYPW78"
      },
      "source": [
        "### From inputs to predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNsHNZa3D-oU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "inputs = tokenizer(example, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu0HMnksD-oV",
        "outputId": "021cbd3c-2b8c-4cb0-dc9e-efa48ce2941d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 19])\n",
              "torch.Size([1, 19, 9])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(inputs[\"input_ids\"].shape)\n",
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9DvEm7ID-oW",
        "outputId": "90d86bc4-eac1-490a-d9c2-37f3b06a2a74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
        "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbqTO3xfD-oX",
        "outputId": "292d703c-6765-431a-83f5-1968b2a5c02b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-MISC',\n",
              " 2: 'I-MISC',\n",
              " 3: 'B-PER',\n",
              " 4: 'I-PER',\n",
              " 5: 'B-ORG',\n",
              " 6: 'I-ORG',\n",
              " 7: 'B-LOC',\n",
              " 8: 'I-LOC'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQlKIDkxD-oY",
        "outputId": "b179d1e8-3a72-4b76-bba9-3bfb1be7604c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},\n",
              " {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},\n",
              " {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},\n",
              " {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},\n",
              " {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},\n",
              " {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},\n",
              " {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},\n",
              " {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = []\n",
        "tokens = inputs.tokens()\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        results.append(\n",
        "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
        "        )\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qcM0g0MD-oZ",
        "outputId": "dccc03e9-5ecb-409b-8084-d6c105ee7096"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),\n",
              " (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "inputs_with_offsets[\"offset_mapping\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbUMPjy4D-oa",
        "outputId": "036535b7-c1cc-4f46-de7f-48c1f8cda503"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "yl"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[12:14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGj1a1v7D-oa",
        "outputId": "0693aede-a40c-4f57-cfc5-755e2d00bd6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},\n",
              " {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},\n",
              " {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},\n",
              " {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},\n",
              " {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},\n",
              " {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},\n",
              " {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},\n",
              " {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        start, end = offsets[idx]\n",
        "        results.append(\n",
        "            {\n",
        "                \"entity\": label,\n",
        "                \"score\": probabilities[idx][pred],\n",
        "                \"word\": tokens[idx],\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMjACzynSqfO"
      },
      "source": [
        "### Grouping entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeDxWgo4D-ob",
        "outputId": "15d02940-b74b-4cd9-f110-66558fd66888"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hugging Face"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example[33:45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7_nAi3dD-oc",
        "outputId": "210eaa27-d132-4464-8fbe-0f8a0d536890"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},\n",
              " {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},\n",
              " {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "idx = 0\n",
        "while idx < len(predictions):\n",
        "    pred = predictions[idx]\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        # Remove the B- or I-\n",
        "        label = label[2:]\n",
        "        start, _ = offsets[idx]\n",
        "\n",
        "        # Grab all the tokens labeled with I-label\n",
        "        all_scores = []\n",
        "        while (\n",
        "            idx < len(predictions)\n",
        "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
        "        ):\n",
        "            all_scores.append(probabilities[idx][pred])\n",
        "            _, end = offsets[idx]\n",
        "            idx += 1\n",
        "\n",
        "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
        "        score = np.mean(all_scores).item()\n",
        "        word = example[start:end]\n",
        "        results.append(\n",
        "            {\n",
        "                \"entity_group\": label,\n",
        "                \"score\": score,\n",
        "                \"word\": word,\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "            }\n",
        "        )\n",
        "    idx += 1\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QCs2f09Xthu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guEQD5aIXtQ8"
      },
      "source": [
        "\n",
        "# Why are fast tokenizers called fast?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEtpnzEwYGWZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKB_o7BnYIGZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "fast_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_with_fast(examples):\n",
        "    return fast_tokenizer(\n",
        "        examples[\"premise\"], examples[\"hypothesis\"], truncation=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du2NJgqoYMY2"
      },
      "outputs": [],
      "source": [
        "slow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n",
        "\n",
        "def tokenize_with_slow(examples):\n",
        "    return fast_tokenizer(\n",
        "        examples[\"premise\"], examples[\"hypothesis\"], truncation=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P16QzTwoYM8q"
      },
      "outputs": [],
      "source": [
        "%time tokenized_datasets = raw_datasets.map(tokenize_with_fast)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abIMc65dYOzb"
      },
      "outputs": [],
      "source": [
        "%time tokenized_datasets = raw_datasets.map(tokenize_with_slow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl0dtWUWYULV"
      },
      "outputs": [],
      "source": [
        "%time tokenized_datasets = raw_datasets.map(tokenize_with_fast, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7NVfRteYZTf"
      },
      "outputs": [],
      "source": [
        "%time tokenized_datasets = raw_datasets.map(tokenize_with_slow, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkYBKd-tYcG6"
      },
      "source": [
        "## Fast tokenizer superpowers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8av1HykYich"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "print(tokenizer(\"Let's talk about tokenizers superpowers.\")[\"input_ids\"])\n",
        "print(tokenizer(\"Let's talk about tokenizers      superpowers.\")[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMSaDbzVYuLs"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer(\"Let's talk about tokenizers superpowers.\")\n",
        "print(encoding.tokens())\n",
        "print(encoding.word_ids())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sScZVE8OYvQE"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer(\n",
        "    \"Let's talk about tokenizers     superpowers.\",\n",
        "    return_offsets_mapping=True\n",
        ")\n",
        "print(encoding.tokens())\n",
        "print(encoding[\"offset_mapping\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZK0MVQyY5zL"
      },
      "source": [
        "## Inside the Token classification pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agModERQZAtx"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "token_classifier = pipeline(\"token-classification\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY1BtSjyZGzn"
      },
      "outputs": [],
      "source": [
        "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
        "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5eGbwt7ZHpQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_checkpoint = \"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
        "inputs = tokenizer(example, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "print(inputs[\"input_ids\"].shape)\n",
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwMOfp55ZKrT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
        "predictions = probabilities.argmax(dim=-1)[0].tolist()\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX-rS0yUZRFd"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4agLcz60ZXGz"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
        "tokens = inputs_with_offsets.tokens()\n",
        "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
        "\n",
        "for idx, pred in enumerate(predictions):\n",
        "    label = model.config.id2label[pred]\n",
        "    if label != \"O\":\n",
        "        start, end = offsets[idx]\n",
        "        results.append(\n",
        "            {\"entity\": label, \"score\": probabilities[idx][pred],\n",
        "             \"word\": tokens[idx], \"start\": start, \"end\": end}\n",
        "        )\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlkLSLu6ZX7G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "label_map = model.config.id2label\n",
        "results = []\n",
        "idx = 0\n",
        "while idx < len(predictions):\n",
        "    pred = predictions[idx]\n",
        "    label = label_map[pred]\n",
        "    if label != \"O\":\n",
        "        # Remove the B- or I-\n",
        "        label = label[2:]\n",
        "        start, _ = offsets[idx]\n",
        "\n",
        "        # Grab all the tokens labeled with I-label\n",
        "        all_scores = []\n",
        "        while idx < len(predictions) and label_map[predictions[idx]] == f\"I-{label}\":\n",
        "            all_scores.append(probabilities[idx][pred])\n",
        "            _, end = offsets[idx]\n",
        "            idx += 1\n",
        "\n",
        "        # The score is the mean of all the scores of the token in that grouped entity.\n",
        "        score = np.mean(all_scores).item()\n",
        "        word = example[start:end]\n",
        "        results.append(\n",
        "            {\"entity_group\": label, \"score\": score,\n",
        "             \"word\": word, \"start\": start, \"end\": end}\n",
        "        )\n",
        "    idx += 1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
