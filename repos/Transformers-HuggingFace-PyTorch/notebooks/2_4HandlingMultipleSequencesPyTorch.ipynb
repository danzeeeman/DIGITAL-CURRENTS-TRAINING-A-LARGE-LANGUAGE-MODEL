{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW2AHYoNZ68z"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL9O2VjytaFP"
      },
      "source": [
        "## Models expect a batch of inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw-PDMUVZ687",
        "outputId": "9618644a-14dd-4023-bc25-c81788f35bce"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)\n",
        "# IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8VGSR3QZ688",
        "outputId": "d2503b29-212a-4145-f8b0-ec9445132ccf"
      },
      "outputs": [],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])\n",
        "# tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
        "#          2607,  2026,  2878,  2166,  1012,   102]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9oYBRHjZ689",
        "outputId": "8763899f-07c4-40a5-8474-657db10675d6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])  # A new dimension added here\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)\n",
        "# Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n",
        "# Logits: [[-2.7276,  2.8789]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O0Kyl2ZtoRF"
      },
      "outputs": [],
      "source": [
        "batched_ids = [ids, ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHCvQVQ9uGtV"
      },
      "source": [
        "## Padding the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INPH7jRnZ68-"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPfoglaWZ68_"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2NOXXO1Z69A",
        "outputId": "8e640662-8c5d-47d7-a20e-774b1eb76769"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)\n",
        "\n",
        "'''tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
        "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
        "tensor([[ 1.5694, -1.3895],\n",
        "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YMZqtIiuVAm"
      },
      "source": [
        "## Attention masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiOhIwXkZ69B",
        "outputId": "41c2fa0d-6143-48f4-b02b-8073d86ce227"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)\n",
        "\n",
        "# tensor([[ 1.5694, -1.3895],\n",
        "#        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKgA1cksucOM"
      },
      "source": [
        "## Longer sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHp7Db1eZ69C"
      },
      "outputs": [],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcaIaiqWue0m"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLHPG4M_u5ss"
      },
      "source": [
        "## Batching inputs together "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNlIWNZRuzkp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\",\n",
        "]\n",
        "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "print(ids[0])\n",
        "print(ids[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLFckh9C5MQA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "ids = [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "       [1045, 5223, 2023, 1012]]\n",
        "\n",
        "input_ids = torch.tensor(ids)\n",
        "# ValueError: expected sequence of length 14 at dim 1 (got 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMZJ8JwW6cMJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "ids = [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "       [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        "\n",
        "input_ids = torch.tensor(ids)\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ey-u02696cuA"
      },
      "outputs": [],
      "source": [
        "### adding padding\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenizer.pad_token_id    # Applying padding here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNbtKAfq7M6k"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "ids1 = torch.tensor(\n",
        "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]]\n",
        ")\n",
        "ids2 = torch.tensor([[1045, 5223, 2023, 1012]])\n",
        "all_ids = torch.tensor(\n",
        "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "     [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "print(model(ids1).logits)\n",
        "print(model(ids2).logits)\n",
        "print(model(all_ids).logits)\n",
        "\n",
        "\"\"\"\n",
        "tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward>)\n",
        "tensor([[ 3.9497, -3.1357]], grad_fn=<AddmmBackward>)\n",
        "tensor([[-2.7276,  2.8789],\n",
        "        [ 1.5444, -1.3998]], grad_fn=<AddmmBackward>)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxWgrh4t6c3i"
      },
      "outputs": [],
      "source": [
        "all_ids = torch.tensor(\n",
        "    [[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012],\n",
        "     [1045, 5223, 2023, 1012,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        ")\n",
        "# adding attention by creating attention mask\n",
        "attention_mask = torch.tensor(\n",
        "    [[   1,    1,    1,    1,    1,    1,    1,     1,     1,    1,    1,    1,    1,    1],\n",
        "     [   1,    1,    1,    1,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeQ1UkwQ7yNb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "output1 = model(ids1)\n",
        "output2 = model(ids2)\n",
        "print(output1.logits)\n",
        "print(output2.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjhsNWCw70HE"
      },
      "outputs": [],
      "source": [
        "output = model(all_ids, attention_mask=attention_mask)\n",
        "print(output.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO_JXLpo8ROe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "sentences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this.\",\n",
        "]\n",
        "print(tokenizer(sentences, padding=True))\n",
        "# {'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n",
        "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS-L59_G8UoO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 (conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1982fbf211ad5dee3df527f8dde59caaf96cdcf1132c131fb0bff5660ddf9aa4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
