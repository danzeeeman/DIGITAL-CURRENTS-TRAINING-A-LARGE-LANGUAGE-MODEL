{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozzUlj6MXsLH"
      },
      "source": [
        "# Training a new tokenizer from an old one"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVlxsyQDXsLN"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4iVyXDIXsLP"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lkwMNnlXsLS"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WyW7w7RXsLT"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg7D8T_lXsLU"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoXuwtW-XsLV"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9YDFr3QXsLW"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOOE_8lbXsLX",
        "outputId": "179af2fb-3c93-430d-f219-c2537647bc2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', \n",
              "      'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', \n",
              "      'func_code_url'\n",
              "    ],\n",
              "    num_rows: 412178\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN5USmHkXsLb"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYQ9GBtWXsLc"
      },
      "outputs": [],
      "source": [
        "# Don't uncomment the following line unless your dataset is small!\n",
        "# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgZqfZs_XsLc"
      },
      "outputs": [],
      "source": [
        "training_corpus = (\n",
        "    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_FYYqgGXsLe",
        "outputId": "ba4fc382-4a04-4ba5-e85d-4d89dd0ea891"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
              "[]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen = (i for i in range(10))\n",
        "print(list(gen))\n",
        "print(list(gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfDCiTe_XsLf"
      },
      "outputs": [],
      "source": [
        "def get_training_corpus():\n",
        "    return (\n",
        "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
        "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
        "    )\n",
        "\n",
        "\n",
        "training_corpus = get_training_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGyBdm8NXsLg"
      },
      "outputs": [],
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_datasets[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzAnJHDA0sta"
      },
      "source": [
        "## Training a new tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HACM0cGcXsLh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ZWAQ-dXsLh",
        "outputId": "e2475f09-2e8e-47af-fe6f-c5f183f49568"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo',\n",
              " 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = '''def add_numbers(a, b):\n",
        "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
        "    return a + b'''\n",
        "\n",
        "tokens = old_tokenizer.tokenize(example)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb7I-znSXsLi"
      },
      "outputs": [],
      "source": [
        "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsMiailkXsLi",
        "outputId": "7219f546-7f23-4151-ebdc-926d26e47c0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ\"\"\"', 'Add', 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`',\n",
              " 'a', '`', 'Ġand', 'Ġ`', 'b', '`.\"\"\"', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizer.tokenize(example)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-KQszTsXsLi",
        "outputId": "169bc41a-0c4a-4a86-ccf8-18bdfa7a2848"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27\n",
              "36"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(tokens))\n",
        "print(len(old_tokenizer.tokenize(example)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8DmYr4KXsLj",
        "outputId": "8152f416-107e-45f7-cc81-28feda6d54be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['class', 'ĠLinear', 'Layer', '():', 'ĊĠĠĠ', 'Ġdef', 'Ġ__', 'init', '__(', 'self', ',', 'Ġinput', '_', 'size', ',',\n",
              " 'Ġoutput', '_', 'size', '):', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'weight', 'Ġ=', 'Ġtorch', '.', 'randn', '(', 'input', '_',\n",
              " 'size', ',', 'Ġoutput', '_', 'size', ')', 'ĊĠĠĠĠĠĠĠ', 'Ġself', '.', 'bias', 'Ġ=', 'Ġtorch', '.', 'zeros', '(',\n",
              " 'output', '_', 'size', ')', 'ĊĊĠĠĠ', 'Ġdef', 'Ġ__', 'call', '__(', 'self', ',', 'Ġx', '):', 'ĊĠĠĠĠĠĠĠ',\n",
              " 'Ġreturn', 'Ġx', 'Ġ@', 'Ġself', '.', 'weights', 'Ġ+', 'Ġself', '.', 'bias', 'ĊĠĠĠĠ']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = \"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias\n",
        "    \"\"\"\n",
        "tokenizer.tokenize(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecbPMXyj0lUa"
      },
      "source": [
        "## Saving the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g93isT2ZXsLk"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTWYqvggXsLk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ60UTgxXsLl"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"code-search-net-tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9ltfNLBXsLm"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"pallavi176/code-search-net-tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ5bnVoF1znA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kucukeeX1zwx"
      },
      "source": [
        "# Training a new tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd4BTjez8Qb-"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\n",
        "  'huggingface-course/bert-base-uncased-tokenizer-without-normalizer'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfvHV85d8Shu"
      },
      "outputs": [],
      "source": [
        "text = \"here is a sentence adapted to our tokenizer\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKxVzpLl8UDH"
      },
      "outputs": [],
      "source": [
        "text = \"এই বাক্যটি আমাদের টোকেনাইজারের উপযুক্ত নয়\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz6zEA6b8WSs"
      },
      "outputs": [],
      "source": [
        "text = \"this tokenizer does not know àccënts and CAPITAL LETTERS\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltdmk7eP8YHx"
      },
      "outputs": [],
      "source": [
        "text = \"the medical vocabulary is divided into many sub-token: paracetamol, phrayngitis\"\n",
        "print(tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LcPpuCaBJGa"
      },
      "outputs": [],
      "source": [
        "AutoTokenizer.train_new_from_iterator(\n",
        "    text_iterator,\n",
        "    vocab_size,\n",
        "    new_special_tokens=None,\n",
        "    special_tokens_map=None,\n",
        "    **kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WPA1W2a8Z0Y"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"code_search_net\", \"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK9tyEEx8bv_"
      },
      "outputs": [],
      "source": [
        "def get_training_corpus():\n",
        "    dataset = raw_datasets[\"train\"]\n",
        "    for start_idx in range(0, len(dataset), 1000):\n",
        "        samples = dataset[start_idx : start_idx + 1000]\n",
        "        yield samples[\"whole_func_string\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qgeYbmh8dhF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "training_corpus = get_training_corpus()\n",
        "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
        "new_tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb3zw96m8gnk"
      },
      "outputs": [],
      "source": [
        "example = \"\"\"class LinearLayer():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weight = torch.randn(input_size, output_size)\n",
        "        self.bias = torch.zeros(output_size)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x @ self.weights + self.bias\n",
        "    \"\"\"\n",
        "\n",
        "print(old_tokenizer.tokenize(example))\n",
        "print(new_tokenizer.tokenize(example))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
