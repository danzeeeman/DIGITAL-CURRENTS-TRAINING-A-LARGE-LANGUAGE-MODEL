{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: huggingface_hub in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (0.20.3)\n",
      "Requirement already satisfied: tqdm in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m961.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from peft) (5.9.8)\n",
      "Collecting torch>=1.13.0 (from peft)\n",
      "  Downloading torch-2.2.0-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-0.27.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Collecting sympy (from torch>=1.13.0->peft)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.13.0->peft)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=1.13.0->peft)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.0->peft)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.13.0->peft)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading peft-0.8.2-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.27.0-py3-none-any.whl (279 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-macosx_11_0_arm64.whl (387 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.4/387.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-macosx_11_0_arm64.whl (393 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.2.0-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-macosx_11_0_arm64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_universal2.whl (18 kB)\n",
      "Installing collected packages: mpmath, xxhash, sympy, safetensors, regex, pyarrow-hotfix, networkx, multidict, MarkupSafe, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, jinja2, aiosignal, torch, aiohttp, transformers, accelerate, peft, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "Successfully installed MarkupSafe-2.1.5 accelerate-0.27.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.17.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2023.10.0 jinja2-3.1.3 mpmath-1.3.0 multidict-6.0.5 multiprocess-0.70.16 networkx-3.2.1 peft-0.8.2 pyarrow-hotfix-0.6 regex-2023.12.25 safetensors-0.4.2 sympy-1.12 torch-2.2.0 transformers-4.37.2 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers peft datasets huggingface_hub tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dantheman/src/DIGITAL-CURRENTS-TRAINING-A-LARGE-LANGUAGE-MODEL/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from huggingface_hub import interpreter_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2011347 examples [00:38, 52595.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# This can take a few minutes to load, so grab a coffee or tea while you wait!\n",
    "raw_datasets = load_dataset(\"json\", data_files=\"../data/2011_2023_phi-2_struct_encoded.json\", field='train', split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"../models/phi-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"text\"][i : i + 1000]\n",
    "        for i in range(0, len(raw_datasets[\"text\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(model_type, trust_remote_code=True)\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 51000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "['Instruct', ':', 'Ġwhat', 'Ġis', 'Ġthe', 'Ġoutcome', 'Ġof', 'Ġ{\"', 'input', '\":', 'Ġ{\"', 'pitcher', '\":', 'Ġ{\"', 'id', '\":', 'Ġ460024', ',', 'Ġ\"', 'name', '\":', 'Ġ\"', 'luke', 'Ġhochevar', '\"},', 'Ġ\"', 'batter', '\":', 'Ġ{\"', 'id', '\":', 'Ġ430895', ',', 'Ġ\"', 'name', '\":', 'Ġ\"', 'maicer', 'Ġizturis', '\"}}}?', 'ĠĊ', 'ĠOutput', ':', 'Ġ{\"', 'result', '\":', 'Ġ{\"', 'event', '\":', 'Ġ\"', 'field', '_', 'out', '\",', 'Ġ\"', 'des', '\":', 'Ġ\"', 'Maicer', 'ĠIzturis', 'Ġgrounds', 'Ġout', 'Ġsoftly', ',', 'Ġsecond', 'Ġbaseman', 'ĠChris', 'ĠGetz', 'Ġto', 'Ġfirst', 'Ġbaseman', 'ĠKila', 'ĠKa', \"'\", 'aihue', '.\"}}', 'Ċ']\n",
      "90\n",
      "['Instruc', 't', ':', 'Ġwhat', 'Ġis', 'Ġthe', 'Ġoutcome', 'Ġof', 'Ġ{\"', 'input', '\":', 'Ġ{\"', 'pitcher', '\":', 'Ġ{\"', 'id', '\":', 'Ġ46', '00', '24', ',', 'Ġ\"', 'name', '\":', 'Ġ\"', 'luke', 'Ġh', 'ochevar', '\"},', 'Ġ\"', 'batter', '\":', 'Ġ{\"', 'id', '\":', 'Ġ430', '895', ',', 'Ġ\"', 'name', '\":', 'Ġ\"', 'maicer', 'Ġi', 'zturis', '\"}}}?', 'Ġ', 'Ċ', 'ĠOutput', ':', 'Ġ{\"', 'result', '\":', 'Ġ{\"', 'event', '\":', 'Ġ\"', 'fi', 'eld', '_', 'out', '\",', 'Ġ\"', 'des', '\":', 'Ġ\"', 'Maicer', 'ĠI', 'zturis', 'Ġgrounds', 'Ġout', 'Ġsoftly', ',', 'Ġsecond', 'Ġbaseman', 'ĠC', 'hris', 'ĠGet', 'z', 'Ġto', 'Ġfi', 'rst', 'Ġbaseman', 'Ġ', 'Kila', 'ĠKa', \"'\", 'aihue', '.\"}}', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "example = '''Instruct: what is the outcome of {\"input\": {\"pitcher\": {\"id\": 460024, \"name\": \"luke hochevar\"}, \"batter\": {\"id\": 430895, \"name\": \"maicer izturis\"}}}? \\n Output: {\"result\": {\"event\": \"field_out\", \"des\": \"Maicer Izturis grounds out softly, second baseman Chris Getz to first baseman Kila Ka\\'aihue.\"}}\\n'''\n",
    "tokens = tokenizer.tokenize(example)\n",
    "old_tokens = old_tokenizer.tokenize(example)\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "print(len(old_tokens))\n",
    "print(old_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding mlb: 60359\n",
      "After adding mlb: 60359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b828a4a9ba34672a3ef15af540ff3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('../models/phi-2-mlb/tokenizer_merged\\\\tokenizer_config.json',\n",
       " '../models/phi-2-mlb/tokenizer_merged\\\\special_tokens_map.json',\n",
       " '../models/phi-2-mlb/tokenizer_merged\\\\vocab.json',\n",
       " '../models/phi-2-mlb/tokenizer_merged\\\\merges.txt',\n",
       " '../models/phi-2-mlb/tokenizer_merged\\\\added_tokens.json',\n",
       " '../models/phi-2-mlb/tokenizer_merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Before adding mlb:\", len(old_tokenizer))\n",
    "\n",
    "tokens_in_mlb_not_in_phio_2 = set(tokenizer.vocab).difference(old_tokenizer.vocab)\n",
    "old_tokenizer.add_tokens(list(tokens_in_mlb_not_in_phio_2))\n",
    "\n",
    "print(\"After adding mlb:\", len(old_tokenizer))\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(model_type)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "old_tokenizer.save_pretrained(\"../models/phi-2-mlb/tokenizer_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/phi-2-mlb\", safe_serialization=True, max_shard_size='4GB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
