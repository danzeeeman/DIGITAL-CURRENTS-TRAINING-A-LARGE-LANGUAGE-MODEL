{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops\n",
    "%pip install peft\n",
    "%pip install trl\n",
    "%pip install tensorboard\n",
    "%pip install -U transformers\n",
    "%pip install -U accelerate datasets \n",
    "%pip install -q https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
    "%pip install tokenizers==0.15.0\n",
    "%pip install torch==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install torchaudio==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install torchvision==0.16.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install transformers==4.35.2\n",
    "%pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset,DatasetDict\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "from trl import SFTTrainer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"json\", data_files='./2011_2023_train_test_valuation.json',field=\"train\", split='all')\n",
    "eval_dataset = load_dataset(\"json\", data_files='./2011_2023_train_test_valuation.json', field=\"valuation\", split='all') \n",
    "test_dataset = load_dataset(\"json\", data_files='./2011_2023_train_test_valuation.json', field=\"test\", split='all')\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\":train_dataset,\n",
    "        \"validation\":eval_dataset,\n",
    "        \"test\":test_dataset\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['train'],\n",
       "        num_rows: 717166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['train'],\n",
       "        num_rows: 79686\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/phi-2\", trust_remote_code=True)\n",
    "## Add Special Tokens\n",
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"../models/phi-2\", \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32, \n",
    "    target_modules = [ \"q_proj\", \"k_proj\", \"v_proj\", \"dense\" ],\n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-specific parameters\n",
    "bs=8    # batch size for training\n",
    "bs_eval=16    # batch size for evaluation\n",
    "ga_steps=16  # gradient accumulation steps\n",
    "lr=0.00002  # learning rate\n",
    "epochs=1\n",
    "\n",
    "steps_per_epoch=len(dataset[\"train\"])//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/phi-2-mlb\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs_eval,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch//2,    # 2 evals per epoch\n",
    "    save_steps=steps_per_epoch//100,\n",
    "    save_total_limit=3,     # save once per epoch\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",      # val_loss will go nan with paged_adamw_8bit\n",
    "    learning_rate=lr,\n",
    "    group_by_length=False,\n",
    "    bf16=True,        \n",
    "    ddp_find_unused_parameters=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"../models/phi-2-mlb/checkpoint-1344\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/phi-2-mlb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:249: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# base model\n",
    "base_path=\"../models/phi-2\"  \n",
    "\n",
    "# adapters: path to folder with adapter_model.safetensors\n",
    "adapter_path=\"../models/phi-2-mlb/\" \n",
    "\n",
    "# # where to save merged model\n",
    "save_to=\"../models/phi-2-mlb/\"       \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path, trust_remote_code=True)\n",
    "## Add Special Tokens\n",
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_path, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.18,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Load LoRA and merge\n",
    "merged_model = PeftModel.from_pretrained(model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(save_to, safe_serialization=True, max_shard_size='4GB')\n",
    "tokenizer.save_pretrained(save_to)\n",
    "generation_config.save_pretrained(save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# The output should be:\n",
      "# {\n",
      "#     \"input\": {\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"farth\",\n",
      "#             \"name\": \"farth\"\n",
      "#         },\n",
      "#         \"farth\": {\n",
      "#             \"id\": \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# model_path=\"../models/phi-2-mlb/\"   \n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_path, \n",
    "#         quantization_config=bnb_config, \n",
    "#         device_map = 'auto',\n",
    "#         trust_remote_code=True,\n",
    "#         use_auth_token=True,\n",
    "#     )\n",
    "\n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prompt= \"Instruct: what is the outcome of {\\\"input\\\": {\\\"pitcher\\\": {\\\"id\\\": 573204, \\\"name\\\": \\\"caleb thielbar\\\"}, \\\"batter\\\": {\\\"id\\\": 488726, \\\"name\\\": \\\"michael brantley\\\"}}}? \\n\"\n",
    "\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_tokens = merged_model.generate(**input_tokens, max_new_tokens=512)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    output_tokens[0][len(input_tokens[0]):],\n",
    "    skip_special_tokens=True\n",
    "    )               \n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
