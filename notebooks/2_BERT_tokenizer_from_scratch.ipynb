{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Ci7MHk-SXf"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ü§ó Tokenizers. Uncomment the following cell and run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX-L8eK29uys",
        "outputId": "c2bc30ff-6d38-48e1-8d88-1ac30ac31fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tokenizer\n",
            "  Downloading tokenizer-3.4.3-py2.py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m873.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizer-3.4.3-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizer\n",
            "Successfully installed tokenizer-3.4.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wglOHRFF-NIH"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from source for both those libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNy3BKI8_FHU"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6di3WY0_GPr",
        "outputId": "52d3b1e1-3270-4356-fb27-bd0ed832e4f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-09 14:12:00--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.167.24, 52.216.62.24, 52.217.84.134, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.167.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòdata/wikitext-103-raw-v1.zip‚Äô\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  9.25MB/s    in 19s     \n",
            "\n",
            "2024-02-09 14:12:19 (9.64 MB/s) - ‚Äòdata/wikitext-103-raw-v1.zip‚Äô saved [191984949/191984949]\n",
            "\n",
            "Archive:  data/wikitext-103-raw-v1.zip\n",
            "   creating: data/wikitext-103-raw/\n",
            "  inflating: data/wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: data/wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: data/wikitext-103-raw/wiki.train.raw  \n"
          ]
        }
      ],
      "source": [
        "# first off we create the data/ dir, download raw wiki-103, and finally unzip the file\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip -P data\n",
        "!unzip data/wikitext-103-raw-v1.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUxQKaFFxOk"
      },
      "source": [
        "## Tokenizer from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abUu182JF4y3"
      },
      "source": [
        "First, BERT relies on WordPiece, so we instantiate a new Tokenizer with this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u0p6_7ZWEJxG"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "bert_tokenizer = Tokenizer(WordPiece())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxOmMAOuGCik"
      },
      "source": [
        "Then we know that BERT preprocesses texts by removing accents and lowercasing. We also use a unicode normalizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EkKr-hZ7F7KI"
      },
      "outputs": [],
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
        "\n",
        "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzONftBqGHha"
      },
      "source": [
        "The pre-tokenizer is just splitting on whitespace and punctuation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f91rOUWvGEwG"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "bert_tokenizer.pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsEmYdeeGLt3"
      },
      "source": [
        "And the post-processing uses the template we saw in the previous section:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OGzt-nPLGJ6Q"
      },
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "bert_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi28Mkv5GRPV"
      },
      "source": [
        "We can use this tokenizer and train on it on wikitext like in the [Quicktour](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OPphQqQoGNzi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")\n",
        "files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "# print(files)\n",
        "bert_tokenizer.train(files=files, trainer=trainer)\n",
        "\n",
        "model_files = bert_tokenizer.model.save(\"data\", \"bert-wiki\")\n",
        "bert_tokenizer.model = WordPiece.from_file(*model_files, unk_token=\"[UNK]\")\n",
        "\n",
        "bert_tokenizer.save(\"data/bert-wiki.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaVtLw1bGh-v"
      },
      "source": [
        "### Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCZsktw5G858"
      },
      "source": [
        "On top of encoding the input texts, a `Tokenizer` also has an API for decoding, that is converting IDs generated by your model back to a text. This is done by the methods `decode()` (for one predicted text) and `decode_batch()` (for a batch of predictions).\n",
        "\n",
        "The decoder will first convert the IDs back to tokens (using the tokenizer‚Äôs vocabulary) and remove all special tokens, then join those tokens with spaces:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9DYi4JrvGdNq",
        "outputId": "b1606843-d315-4379-adcc-a0e525543063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"hello , y ' all ! how are you ?\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = bert_tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "print(output.ids)\n",
        "# [1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2]\n",
        "\n",
        "bert_tokenizer.decode([1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2])\n",
        "# \"Hello , y ' all ! How are you ?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDq7j5NIHjoV"
      },
      "source": [
        "If you used a model that added special characters to represent subtokens of a given ‚Äúword‚Äù (like the `\"##\"` in WordPiece) you will need to customize the decoder to treat them properly. If we take our previous `bert_tokenizer` for instance the default decoing will give:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9xmzSDRhHSLc",
        "outputId": "a646c076-c7a0-40e7-bac3-944b27012aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'welcome', 'to', 'the', '[UNK]', 'tok', '##eni', '##zer', '##s', 'library', '.', '[SEP]']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'welcome to the tok ##eni ##zer ##s library .'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = bert_tokenizer.encode(\"Welcome to the ü§ó Tokenizers library.\")\n",
        "print(output.tokens)\n",
        "# [\"[CLS]\", \"welcome\", \"to\", \"the\", \"[UNK]\", \"tok\", \"##eni\", \"##zer\", \"##s\", \"library\", \".\", \"[SEP]\"]\n",
        "\n",
        "bert_tokenizer.decode(output.ids)\n",
        "# \"welcome to the tok ##eni ##zer ##s library .\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTQpXgIZH-Y-"
      },
      "source": [
        "But by changing it to a proper decoder, we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h4jZekYhH5uG",
        "outputId": "65887239-a72c-4784-ae4d-0207b3127e63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'welcome to the tokenizers library.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tokenizers import decoders\n",
        "\n",
        "bert_tokenizer.decoder = decoders.WordPiece()\n",
        "bert_tokenizer.decode(output.ids)\n",
        "# \"welcome to the tokenizers library.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DirAX_BxICUR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "BERT tokenizer from scratch",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
