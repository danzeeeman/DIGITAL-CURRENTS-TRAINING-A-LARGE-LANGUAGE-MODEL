{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qFy4Eqib2cdV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### General note for GPU training (in colab)\n",
        "\n",
        "* First, please use the GPU runtime. If so the `!nvidia-smi` will return no error.\n",
        "  1. Click on \"Runtime\" in the top menu bar.\n",
        "  2. Select \"Change runtime type\" from the drop-down menu.\n",
        "  3. In the \"Runtime type\" section, select \"GPU\" as the hardware accelerator.\n",
        "  4. Click \"Save\" to apply the changes.\n",
        "\n",
        "\n",
        "* What should I do with **Cuda out of memory error.**? (this is THE mode common error in DL)\n",
        "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*enMsxkgJ1eb9XvtWju5V8Q.png)\n",
        "  1. In colab notebook, **unfortunately, you need to restart the kernel after OOM happened**. Or it will keep happening no matter what.\n",
        "  2. Change the model to save memory, usually includes, decrease batch size, decrease the number of layers, decrease the max sequence length, decrease the hidden / embedding dimension\n",
        "  3. If you know mixed precision training, you can switch to low precision `fp16` numbers for weights and inputs.\n",
        "\n",
        "* What should I do for the **Device siee assert triggered** error\n",
        "  > RuntimeError: CUDA error: device-side assert triggered\n",
        "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
        "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
        "  \n",
        "  * Usually it's because the embedding layer receive an index (token id or position id) not stored in it.\n",
        "  * Could be sth. else, which will be harder to debug..."
      ],
      "metadata": {
        "id": "qFy4Eqib2cdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\" # to fix a potential locale bug\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "9gN2GMtC263o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "thhCXiFZ6aJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  transformers torchaudio"
      ],
      "metadata": {
        "id": "l0jnJY-cMIu8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8491a2-4665-49a9-d83e-a83bb6ea3691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.0.1+cu118)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torchaudio) (2.0.0+cu118)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchaudio) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchaudio) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchaudio) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchaudio) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchaudio) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchaudio) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchaudio) (3.25.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0->torchaudio) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0->torchaudio) (1.3.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchfsdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caMV_JgY9dxF",
        "outputId": "1f74996c-a46a-4e06-ee65-3947e9ccd28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchfsdd\n",
            "  Using cached torchfsdd-1.0.0.tar.gz (11 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwJArTSgLtJ8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2Model\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchfsdd import TorchFSDDGenerator, TrimSilence\n",
        "from torchaudio.transforms import MFCC\n",
        "from torchvision.transforms import Compose"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transformation pipeline to apply to the recordings\n",
        "transforms = Compose([\n",
        "    TrimSilence(threshold=1e-6),\n",
        "    MFCC(sample_rate=8e3, n_mfcc=64)\n",
        "])\n",
        "\n",
        "# Fetch the latest version of FSDD and initialize a generator with those files\n",
        "fsdd = TorchFSDDGenerator(version='master', transforms=transforms,)\n",
        "# Create a Torch dataset for the entire dataset from the generator\n",
        "full_set = fsdd.full()\n",
        "# Create two Torch datasets for a train-test split from the generator\n",
        "train_set, test_set = fsdd.train_test_split(test_size=0.1)\n",
        "# Create three Torch datasets for a train-validation-test split from the generator\n",
        "train_set, val_set, test_set = fsdd.train_val_test_split(test_size=0.15, val_size=0.15)"
      ],
      "metadata": {
        "id": "vod8-Sh6L_4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(np.log(np.abs(train_set[100][0])))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y7cfmAUxMD_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # batch is a list of tuples, where each tuple is (audio_tensor, label_scalar)\n",
        "    audios = []\n",
        "    labels = []\n",
        "    for audio, label in batch:\n",
        "        audios.append(audio.T)  # time, freq features\n",
        "        labels.append(label)\n",
        "    # pad audio tensors to ensure they have the same length\n",
        "    audios = pad_sequence(audios, batch_first=True, padding_value=0)\n",
        "    # convert the labels list to a tensor\n",
        "    labels = torch.tensor(labels)\n",
        "    return audios, labels\n",
        "\n",
        "\n",
        "audio_tsrs, labels = next(iter(dataloaders))\n",
        "print(audio_tsrs.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "A7AElT3zMFOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT version"
      ],
      "metadata": {
        "id": "Ifk3iik7Mi28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2Model\n",
        "config = GPT2Config(n_embd=128, n_layer=12, n_head=16, n_positions=256,\n",
        "                    vocab_size=100, bos_token_id=101, eos_token_id=102,\n",
        "                    cls_token_id=103, )\n",
        "MF_emb = nn.Linear(64, config.n_embd).cuda()\n",
        "model = GPT2Model(config).cuda()\n",
        "classifier_head = nn.Linear(config.n_embd, 10).cuda()\n",
        "CLS_token = torch.randn(1, 1, config.n_embd).cuda() / math.sqrt(config.n_embd)\n",
        "CLS_token = nn.Parameter(CLS_token)\n",
        "optimizer = AdamW([*model.parameters(),\n",
        "                  *MF_emb.parameters(),\n",
        "                  *classifier_head.parameters(),\n",
        "                   CLS_token], lr=1e-4)\n"
      ],
      "metadata": {
        "id": "BhdZr92VL456"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataloaders = DataLoader(train_set, batch_size=128, shuffle=True,\n",
        "                         collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True,\n",
        "                            collate_fn=collate_fn)\n",
        "for epoch in trange(20):\n",
        "    model.train()\n",
        "    pbar = tqdm(dataloaders)\n",
        "    for i, (audio, label) in enumerate(pbar):\n",
        "        audio = audio.cuda()\n",
        "        audio = MF_emb(audio)\n",
        "        audio = torch.cat([audio, CLS_token.repeat(audio.shape[0], 1, 1)], dim=1)\n",
        "        output = model(inputs_embeds=audio)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        pooled_output = last_hidden_state[:, -1]\n",
        "        logits = classifier_head(pooled_output)\n",
        "        loss = F.cross_entropy(logits, label.cuda())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f\"loss: {loss.item():.4f}\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_corr_num = 0\n",
        "        test_loss = 0\n",
        "        for i, (audio, label) in enumerate(test_loader):\n",
        "            audio = audio.cuda()\n",
        "            audio = MF_emb(audio)\n",
        "            audio = torch.cat([audio, CLS_token.repeat(audio.shape[0], 1, 1)], dim=1)\n",
        "            output = model(inputs_embeds=audio)\n",
        "            last_hidden_state = output.last_hidden_state\n",
        "            pooled_output = last_hidden_state[:, -1]\n",
        "            logits = classifier_head(pooled_output)\n",
        "            loss = F.cross_entropy(logits, label.cuda())\n",
        "            pbar.set_description(f\"test loss: {loss.item():.4f}\")\n",
        "            test_corr_num += (logits.argmax(dim=1) == label.cuda()).float().sum()\n",
        "            test_loss += loss.item()\n",
        "        print(f\"test acc: {test_corr_num / len(test_set):.4f}\")\n"
      ],
      "metadata": {
        "id": "Hvuf-kOqMRx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT version"
      ],
      "metadata": {
        "id": "5LofALAZMgF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer, BertConfig\n",
        "config = BertConfig(hidden_size=64, intermediate_size=256, num_hidden_layers=12,\n",
        "                    num_attention_heads=8, max_position_embeddings=256,\n",
        "                    vocab_size=100, bos_token_id=101, eos_token_id=102,\n",
        "                    cls_token_id=103, )\n",
        "model = BertModel(config).cuda()\n",
        "# MF_emb = nn.Linear(64, config.hidden_size).cuda()\n",
        "MF_emb = nn.Sequential(nn.Conv1d(64, config.hidden_size, 3, 1, 1),\n",
        "                       nn.ReLU(),\n",
        "                       nn.Conv1d(config.hidden_size, config.hidden_size, 3, 1, 1),\n",
        "                       ).cuda()\n",
        "classifier_head = nn.Linear(config.hidden_size, 10).cuda()\n",
        "CLS_token = torch.randn(1, 1, config.hidden_size).cuda() / math.sqrt(config.hidden_size)\n",
        "CLS_token = nn.Parameter(CLS_token)\n",
        "optimizer = AdamW([*model.parameters(),\n",
        "                  *MF_emb.parameters(),\n",
        "                  *classifier_head.parameters(),\n",
        "                   CLS_token], lr=1e-4)\n",
        "# https://datasets.activeloop.ai/docs/ml/datasets/free-spoken-digit-dataset-fsdd/\n",
        "# https://github.com/adhishthite/sound-mnist"
      ],
      "metadata": {
        "id": "eLtaN0qSL3gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = DataLoader(train_set, batch_size=128, shuffle=True,\n",
        "                         collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_set, batch_size=256, shuffle=True,\n",
        "                            collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=256, shuffle=True,\n",
        "                            collate_fn=collate_fn)\n",
        "for epoch in trange(40):\n",
        "    model.train()\n",
        "    pbar = tqdm(dataloaders)\n",
        "    for i, (audio, label) in enumerate(pbar):\n",
        "        audio = audio.cuda()\n",
        "        audio = MF_emb(audio.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        audio = torch.cat([CLS_token.repeat(audio.shape[0], 1, 1), audio, ], dim=1)\n",
        "        output = model(inputs_embeds=audio)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        pooled_output = last_hidden_state[:, 0]\n",
        "        logits = classifier_head(pooled_output)\n",
        "        loss = F.cross_entropy(logits, label.cuda())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f\"loss: {loss.item():.4f}\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_corr_num = 0\n",
        "        val_loss = 0\n",
        "        for i, (audio, label) in enumerate(val_loader):\n",
        "            audio = audio.cuda()\n",
        "            audio = MF_emb(audio.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            audio = torch.cat([CLS_token.repeat(audio.shape[0], 1, 1), audio, ], dim=1)\n",
        "            output = model(inputs_embeds=audio)\n",
        "            last_hidden_state = output.last_hidden_state\n",
        "            pooled_output = last_hidden_state[:, 0]\n",
        "            logits = classifier_head(pooled_output)\n",
        "            loss = F.cross_entropy(logits, label.cuda())\n",
        "            val_corr_num += (logits.argmax(dim=1) == label.cuda()).float().sum()\n",
        "            val_loss += loss.item()\n",
        "        print(f\"val acc: {val_corr_num / len(val_set):.4f}\")\n",
        "\n",
        "        test_corr_num = 0\n",
        "        test_loss = 0\n",
        "        for i, (audio, label) in enumerate(test_loader):\n",
        "            audio = audio.cuda()\n",
        "            audio = MF_emb(audio.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "            audio = torch.cat([CLS_token.repeat(audio.shape[0], 1, 1), audio, ], dim=1)\n",
        "            output = model(inputs_embeds=audio)\n",
        "            last_hidden_state = output.last_hidden_state\n",
        "            pooled_output = last_hidden_state[:, 0]\n",
        "            logits = classifier_head(pooled_output)\n",
        "            loss = F.cross_entropy(logits, label.cuda())\n",
        "            test_corr_num += (logits.argmax(dim=1) == label.cuda()).float().sum()\n",
        "            test_loss += loss.item()\n",
        "        print(f\"test acc: {test_corr_num / len(test_set):.4f}\")\n"
      ],
      "metadata": {
        "id": "PhZnpvNuL1Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "loss: 0.0476: 100%|██████████| 17/17 [00:28<00:00,  1.66s/it]\n",
        "val acc: 0.9833\n",
        "test acc: 0.9714\n",
        "100%|██████████| 40/40 [25:56<00:00, 38.92s/it]\n",
        "```"
      ],
      "metadata": {
        "id": "v6TLFxr-MZoi"
      }
    }
  ]
}