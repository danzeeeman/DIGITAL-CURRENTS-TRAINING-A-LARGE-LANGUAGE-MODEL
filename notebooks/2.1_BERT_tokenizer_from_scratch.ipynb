{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT tokenizer from scratch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Ci7MHk-SXf"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ü§ó Tokenizers. Uncomment the following cell and run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX-L8eK29uys",
        "outputId": "c2bc30ff-6d38-48e1-8d88-1ac30ac31fee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#! pip install tokenizers===0.9.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers===0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.9MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.9.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wglOHRFF-NIH"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from source for both those libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNy3BKI8_FHU"
      },
      "source": [
        "## Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6di3WY0_GPr",
        "outputId": "52d3b1e1-3270-4356-fb27-bd0ed832e4f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# first off we create the data/ dir, download raw wiki-103, and finally unzip the file\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip -P data\n",
        "!unzip data/wikitext-103-raw-v1.zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‚Äòdata‚Äô: File exists\n",
            "--2020-11-07 00:20:54--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.136.29\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.136.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191984949 (183M) [application/zip]\n",
            "Saving to: ‚Äòdata/wikitext-103-raw-v1.zip.1‚Äô\n",
            "\n",
            "wikitext-103-raw-v1 100%[===================>] 183.09M  75.7MB/s    in 2.4s    \n",
            "\n",
            "2020-11-07 00:20:57 (75.7 MB/s) - ‚Äòdata/wikitext-103-raw-v1.zip.1‚Äô saved [191984949/191984949]\n",
            "\n",
            "Archive:  data/wikitext-103-raw-v1.zip\n",
            "   creating: data/wikitext-103-raw/\n",
            "  inflating: data/wikitext-103-raw/wiki.test.raw  \n",
            "  inflating: data/wikitext-103-raw/wiki.valid.raw  \n",
            "  inflating: data/wikitext-103-raw/wiki.train.raw  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmUxQKaFFxOk"
      },
      "source": [
        "## Tokenizer from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abUu182JF4y3"
      },
      "source": [
        "First, BERT relies on WordPiece, so we instantiate a new Tokenizer with this model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0p6_7ZWEJxG"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "bert_tokenizer = Tokenizer(WordPiece())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxOmMAOuGCik"
      },
      "source": [
        "Then we know that BERT preprocesses texts by removing accents and lowercasing. We also use a unicode normalizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkKr-hZ7F7KI"
      },
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
        "\n",
        "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzONftBqGHha"
      },
      "source": [
        "The pre-tokenizer is just splitting on whitespace and punctuation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91rOUWvGEwG"
      },
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "bert_tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsEmYdeeGLt3"
      },
      "source": [
        "And the post-processing uses the template we saw in the previous section:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGzt-nPLGJ6Q"
      },
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "bert_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi28Mkv5GRPV"
      },
      "source": [
        "We can use this tokenizer and train on it on wikitext like in the [Quicktour](https://huggingface.co/docs/tokenizers/python/latest/quicktour.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPphQqQoGNzi"
      },
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")\n",
        "files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
        "bert_tokenizer.train(trainer, files)\n",
        "\n",
        "model_files = bert_tokenizer.model.save(\"data\", \"bert-wiki\")\n",
        "bert_tokenizer.model = WordPiece.from_file(*model_files, unk_token=\"[UNK]\")\n",
        "\n",
        "bert_tokenizer.save(\"data/bert-wiki.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaVtLw1bGh-v"
      },
      "source": [
        "### Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCZsktw5G858"
      },
      "source": [
        "On top of encoding the input texts, a `Tokenizer` also has an API for decoding, that is converting IDs generated by your model back to a text. This is done by the methods `decode()` (for one predicted text) and `decode_batch()` (for a batch of predictions).\n",
        "\n",
        "The decoder will first convert the IDs back to tokens (using the tokenizer‚Äôs vocabulary) and remove all special tokens, then join those tokens with spaces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DYi4JrvGdNq",
        "outputId": "b1606843-d315-4379-adcc-a0e525543063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "output = bert_tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "print(output.ids)\n",
        "# [1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2]\n",
        "\n",
        "bert_tokenizer.decode([1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2])\n",
        "# \"Hello , y ' all ! How are you ?\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 27462, 16, 67, 11, 7323, 5, 7510, 7268, 7989, 0, 35, 2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"hello , y ' all ! how are you ?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDq7j5NIHjoV"
      },
      "source": [
        "If you used a model that added special characters to represent subtokens of a given ‚Äúword‚Äù (like the `\"##\"` in WordPiece) you will need to customize the decoder to treat them properly. If we take our previous `bert_tokenizer` for instance the default decoing will give:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmzSDRhHSLc",
        "outputId": "a646c076-c7a0-40e7-bac3-944b27012aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "output = bert_tokenizer.encode(\"Welcome to the ü§ó Tokenizers library.\")\n",
        "print(output.tokens)\n",
        "# [\"[CLS]\", \"welcome\", \"to\", \"the\", \"[UNK]\", \"tok\", \"##eni\", \"##zer\", \"##s\", \"library\", \".\", \"[SEP]\"]\n",
        "\n",
        "bert_tokenizer.decode(output.ids)\n",
        "# \"welcome to the tok ##eni ##zer ##s library .\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'welcome', 'to', 'the', '[UNK]', 'tok', '##eni', '##zer', '##s', 'library', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'welcome to the tok ##eni ##zer ##s library .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTQpXgIZH-Y-"
      },
      "source": [
        "But by changing it to a proper decoder, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4jZekYhH5uG",
        "outputId": "65887239-a72c-4784-ae4d-0207b3127e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from tokenizers import decoders\n",
        "\n",
        "bert_tokenizer.decoder = decoders.WordPiece()\n",
        "bert_tokenizer.decode(output.ids)\n",
        "# \"welcome to the tokenizers library.\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'welcome to the tokenizers library.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DirAX_BxICUR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}