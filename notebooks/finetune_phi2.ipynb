{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install einops\n",
    "%pip install peft\n",
    "%pip install trl\n",
    "%pip install tensorboard\n",
    "%pip install -q -U https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/danm/.cache/huggingface/datasets/json/default-5f8cce314afb587c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./2011_2023_text.json\", field='data', split='all')\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 717166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 79686\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6d3ad5741948408741289d9afb4145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"../models/phi-2\", \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32, \n",
    "    target_modules = [ \"q_proj\", \"k_proj\", \"v_proj\", \"dense\" ],\n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj','dense','fc1','fc2',]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# dataset-specific parameters\n",
    "bs=2     # batch size for training\n",
    "bs_eval=16    # batch size for evaluation\n",
    "ga_steps=16  # gradient accumulation steps\n",
    "lr=0.00002  # learning rate\n",
    "epochs=1\n",
    "\n",
    "steps_per_epoch=len(dataset[\"train\"])//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/phi-2-mlb\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs_eval,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch//2,    # 2 evals per epoch\n",
    "    save_steps=steps_per_epoch//100,\n",
    "    save_total_limit=3,     # save once per epoch\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",      # val_loss will go nan with paged_adamw_8bit\n",
    "    learning_rate=lr,\n",
    "    group_by_length=False,\n",
    "    bf16=True,        \n",
    "    ddp_find_unused_parameters=False,\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e30e92792f409da4598b3e8dad46d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/717166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:258\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m     dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_dataset(\n\u001b[0;32m    259\u001b[0m         train_dataset,\n\u001b[0;32m    260\u001b[0m         tokenizer,\n\u001b[0;32m    261\u001b[0m         packing,\n\u001b[0;32m    262\u001b[0m         dataset_text_field,\n\u001b[0;32m    263\u001b[0m         max_seq_length,\n\u001b[0;32m    264\u001b[0m         formatting_func,\n\u001b[0;32m    265\u001b[0m         num_of_sequences,\n\u001b[0;32m    266\u001b[0m         chars_per_token,\n\u001b[0;32m    267\u001b[0m         remove_unused_columns\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mremove_unused_columns \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs,\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m     _multiple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:371\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_dataset\u001b[1;34m(self, dataset, tokenizer, packing, dataset_text_field, max_seq_length, formatting_func, num_of_sequences, chars_per_token, remove_unused_columns, append_concat_token, add_special_tokens)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packing:\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_non_packed_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_packed_dataloader(\n\u001b[0;32m    383\u001b[0m         tokenizer,\n\u001b[0;32m    384\u001b[0m         dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m         add_special_tokens,\n\u001b[0;32m    392\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:439\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader\u001b[1;34m(self, tokenizer, dataset, dataset_text_field, max_seq_length, formatting_func, add_special_tokens, remove_unused_columns)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remove_unused_columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(extra_colmuns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    434\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minspect dataset other columns (in this case \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_colmuns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), you can subclass `DataCollatorForLanguageModeling` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m     )\n\u001b[1;32m--> 439\u001b[0m tokenized_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_num_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_dataset\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 563\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    564\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    526\u001b[0m }\n\u001b[0;32m    527\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 528\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    529\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:2953\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2946\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   2947\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   2948\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2951\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2952\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 2953\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   2954\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   2955\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:3329\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3325\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3326\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3327\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3328\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3329\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3333\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3335\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3338\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:3210\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3209\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3210\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3212\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3213\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3214\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:410\u001b[0m, in \u001b[0;36mSFTTrainer._prepare_non_packed_dataloader.<locals>.tokenize\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(element):\n\u001b[0;32m    409\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m--> 410\u001b[0m         \u001b[43melement\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_formatting_func \u001b[38;5;28;01melse\u001b[39;00m formatting_func(element),\n\u001b[0;32m    411\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m    412\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    413\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    414\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[0;32m    415\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    416\u001b[0m         return_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_formatting_func \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_sanity_checked:\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formatting_func(element), \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\datasets\\formatting\\formatting.py:280\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 280\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m    282\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844aa585f83e4e7ba4851daccfac09b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8227, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7298, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7161, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7308, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7457, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7392, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6812, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6583, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6329, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5237, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6116, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5893, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5667, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5725, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5247, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5128, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4649, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4071, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4558, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4403, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3882, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4108, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3063, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3012, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3227, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.2909, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.2996, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.226, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1915, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.183, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1882, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.2041, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1628, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1568, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1538, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0559, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0581, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0751, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0415, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0249, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0458, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9914, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.987, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.023, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9193, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9509, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9229, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9167, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9192, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8298, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8145, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8163, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7722, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8246, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7722, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8207, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7817, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7921, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6831, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7704, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7049, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7258, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7418, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6973, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6739, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6819, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6358, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.651, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6684, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6771, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6156, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6228, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6455, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.645, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5795, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6139, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6158, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.639, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6615, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6148, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5556, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5966, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5661, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6067, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.596, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5679, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6296, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6247, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5694, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5526, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5772, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5581, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6225, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5676, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5312, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5464, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5389, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.579, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5222, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4668, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6003, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.55, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4712, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5489, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5391, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5489, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5364, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5623, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4937, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.546, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5091, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5223, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5554, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5042, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5563, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5352, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4772, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5144, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4877, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5237, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5034, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4663, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5323, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5568, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5079, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4939, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5235, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5114, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4936, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5251, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5047, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5028, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5191, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5371, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5079, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4969, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4777, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4786, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4819, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5019, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4804, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4893, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5008, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4804, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5087, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5142, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4992, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.472, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4783, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4863, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4963, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4804, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4983, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4438, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.449, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4728, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4603, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5064, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4893, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4467, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5217, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.493, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5076, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4699, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4446, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5191, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4673, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5011, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5047, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4742, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4984, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4424, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4887, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4575, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.444, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4835, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4732, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4637, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4762, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5245, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4638, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4825, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4361, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4372, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4745, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4672, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4556, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4921, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4429, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4437, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4676, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4291, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4796, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4797, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4609, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4454, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4864, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4788, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4569, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4465, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4428, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.486, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4423, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4496, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4134, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4278, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4247, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4513, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4745, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4828, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4431, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4884, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4757, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4848, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4433, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4616, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4563, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4623, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4416, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4705, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4434, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4357, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4454, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4397, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4802, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4258, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4542, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4445, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.456, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.467, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4714, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4141, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4432, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4256, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4389, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4318, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4626, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4246, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4508, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.455, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4312, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4742, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4333, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4384, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4357, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.448, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.447, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4433, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4383, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4316, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4616, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4307, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4048, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.395, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4562, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4316, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4232, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4527, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4631, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4214, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4385, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4629, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4397, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4241, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4299, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4532, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4525, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4245, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4215, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4224, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4511, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4303, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.422, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4215, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4376, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4228, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.454, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4103, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4259, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4123, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4509, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3892, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4159, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.459, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4344, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.42, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3777, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4188, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4188, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4127, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4413, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4485, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4011, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4429, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4157, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.405, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.429, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4152, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4171, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4424, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4125, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.413, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4622, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4251, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4088, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.424, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3998, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4441, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4002, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4154, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4278, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4388, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4212, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4283, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4463, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4075, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4336, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4318, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4504, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4156, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4302, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4015, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.466, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.409, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4277, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4448, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4114, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4366, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3991, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4321, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.43, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4146, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4343, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4151, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4164, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4204, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4462, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4225, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4156, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4436, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4149, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4136, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4255, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4357, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4249, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4351, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4232, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.433, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.401, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4144, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3873, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4116, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4052, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4275, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4026, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4171, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4262, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4295, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4643, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.425, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4479, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4139, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4209, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4168, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4051, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3736, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4147, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4316, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4345, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4056, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3818, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4261, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4135, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4488, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4093, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.409, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3979, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4371, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4118, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.436, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4016, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4314, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4427, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4291, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4034, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3988, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3899, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3959, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3993, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3742, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3799, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.401, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4085, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.392, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4473, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4181, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4389, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4307, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4053, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4037, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3934, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3925, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4079, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4069, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3999, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3725, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4214, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.394, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.388, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4059, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4129, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3992, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4189, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4244, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4146, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3591, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4289, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3653, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3859, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4374, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3965, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3568, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4032, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3696, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3939, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3898, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4188, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3987, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3761, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3726, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4425, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4068, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3919, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3817, 'learning_rate': 2e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4009, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.373, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.405, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3918, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4515, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4158, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4069, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3862, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3694, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4079, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3902, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4093, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4045, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3966, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3875, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4133, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3973, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3992, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4293, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3934, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3799, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3831, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3882, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3894, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3818, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3872, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3953, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3959, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3974, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4108, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4145, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3752, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4069, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3727, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3951, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3986, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3822, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3899, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3907, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3837, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.398, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4259, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3786, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3825, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4114, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3871, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3931, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3705, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3838, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3864, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3895, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3818, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3721, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3784, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3884, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3891, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3886, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4102, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3778, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3958, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3805, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4049, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3776, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3828, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3796, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.397, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3607, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3724, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3849, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4087, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3927, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3893, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.406, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3842, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3956, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3639, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4054, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.402, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3798, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3806, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4029, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3859, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3813, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3829, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3961, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3897, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3975, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3765, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3864, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3913, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3987, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.357, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3797, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3771, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4064, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.389, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3677, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3456, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3914, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4165, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4322, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3381, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3991, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3859, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3811, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3647, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.392, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3781, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3884, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3853, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3849, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3682, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3752, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3584, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3817, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.383, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3786, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3865, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3782, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.41, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3618, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3819, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3756, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4114, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3673, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3846, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3573, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3593, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3705, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3566, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4043, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3637, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.403, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3707, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3855, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3787, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.386, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3948, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.396, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3726, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3786, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3967, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.378, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3762, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4001, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.404, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3638, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3735, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3977, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3894, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3809, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3908, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3755, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3632, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3792, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3629, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3898, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3723, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4049, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3695, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3745, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3862, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.365, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.356, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3843, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3651, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3542, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3948, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3742, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3812, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3642, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3722, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3565, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3708, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3585, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.363, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3586, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3603, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3395, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3794, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3892, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3993, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3686, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3984, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4079, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3834, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.411, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3583, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3532, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3825, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3567, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3718, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3608, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3835, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4081, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3541, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3505, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3748, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3767, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3689, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.377, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3716, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3885, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3847, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3695, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3553, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3689, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3967, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3891, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3757, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.366, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3744, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3649, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3641, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3657, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3696, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3812, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3694, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.406, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4011, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3923, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3688, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3944, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3602, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3506, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3677, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.357, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.351, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3637, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3731, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3902, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.398, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4071, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3605, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3638, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3815, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3595, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3868, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3764, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3699, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3676, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3764, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3761, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3785, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3909, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3675, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3852, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3645, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3691, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3557, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3728, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3561, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3556, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3488, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3482, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3769, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3691, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3841, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3744, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3798, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.372, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3499, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4035, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3496, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3649, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.383, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3564, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3922, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3697, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3479, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3444, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3465, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3533, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3857, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3712, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3569, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3603, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3467, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3534, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.355, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3436, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3514, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3655, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3654, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3827, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3457, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3831, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3887, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3657, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3734, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3735, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3642, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3537, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.366, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3727, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3547, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.347, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3895, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3601, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3641, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3372, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4003, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3611, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3441, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3477, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.382, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3571, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3203, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3596, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4154, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3376, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3622, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3685, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3476, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3354, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3666, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3796, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3806, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3529, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3699, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3744, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3525, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3692, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3608, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3736, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3426, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3487, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3746, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.356, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3366, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3536, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3547, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.345, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.355, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3593, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3329, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3822, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.361, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3739, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3607, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3481, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3871, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3549, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3533, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3636, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3469, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3498, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3391, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3612, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3615, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.38, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.344, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.338, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3324, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3454, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3323, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.343, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3476, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3601, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3538, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3738, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3632, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3958, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3197, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3785, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.379, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3409, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3338, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3839, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3492, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3662, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3647, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3781, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3469, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3321, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3734, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3553, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.346, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.353, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3941, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3586, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3552, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3488, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3598, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3394, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3688, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3688, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3577, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3721, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3491, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3558, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3819, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3628, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3389, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3335, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3356, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3491, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3546, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.367, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3355, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3616, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3826, 'learning_rate': 2e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3931, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3742, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3525, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3438, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3475, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3457, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3486, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3262, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3561, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3595, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3461, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3405, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3551, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.37, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3356, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3558, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3617, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3671, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3522, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3599, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.372, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3331, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3783, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3512, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3441, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3406, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3563, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3477, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3769, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3578, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3765, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3499, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3499, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3391, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3332, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3513, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3411, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3459, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3443, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3361, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3564, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3507, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3462, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3267, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3618, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3911, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.346, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3293, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3463, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3376, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3126, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3559, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3271, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3319, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3544, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3682, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3459, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3429, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3523, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3717, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3474, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3582, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3296, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3538, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3361, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3538, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3829, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3573, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3403, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3482, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3653, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3545, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3346, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3321, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.345, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3297, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3412, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.349, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3292, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3556, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3378, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3467, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3516, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3478, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3428, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3361, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3357, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3581, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3302, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3543, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3525, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3632, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3437, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3415, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3661, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3395, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3812, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3491, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3627, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3284, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3528, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3451, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3293, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3442, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3238, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3841, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3451, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3401, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3373, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3679, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3226, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3787, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3582, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3411, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3573, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.351, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3358, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3442, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3379, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3539, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3615, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3408, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.364, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3393, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3355, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3383, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3418, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3292, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3424, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3508, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3682, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3573, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3335, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.35, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3443, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3446, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3598, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3513, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.329, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3385, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3443, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3841, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3697, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3254, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3618, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3432, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3216, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3647, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3356, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3559, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.348, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3641, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3517, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3212, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3501, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.338, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3474, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3525, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3391, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3351, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3457, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3415, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3443, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3322, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3562, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3509, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3652, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3549, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3567, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3446, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3441, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3342, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3485, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3397, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3477, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3631, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3533, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3514, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3517, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3171, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3574, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3502, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3262, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3557, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3226, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3387, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3593, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3279, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3481, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3551, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3402, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3425, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3291, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3606, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3298, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3277, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3529, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3421, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.335, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3548, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3544, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.326, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3629, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3514, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3315, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.309, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3288, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3387, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3394, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3676, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.348, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3368, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3419, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3354, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3428, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3167, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3395, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3248, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3615, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3611, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3469, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3425, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3478, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3191, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3278, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.315, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.332, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3287, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3586, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.341, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3348, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3486, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3487, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3138, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3295, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3541, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3153, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3265, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3445, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3271, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3273, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3482, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3304, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3185, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3101, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3187, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3786, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3392, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3679, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3288, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3383, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3556, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3475, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3476, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.32, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.31, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.362, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3454, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3271, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3431, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.317, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3389, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.342, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3181, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3602, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3295, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3243, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3373, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.329, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3396, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.356, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3497, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.328, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3442, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.334, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3657, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3378, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.364, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3418, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3379, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3151, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3609, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3459, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3555, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3588, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3262, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3264, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3481, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3276, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3337, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3431, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3598, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3378, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3648, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3712, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3357, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3629, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3403, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3211, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3378, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3412, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3612, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3474, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3091, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3509, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3453, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3329, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3564, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3551, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.354, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3157, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3194, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3482, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.332, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3551, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3266, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3701, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2969, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3399, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3335, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3286, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3283, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3326, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3168, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3419, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3189, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3674, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3291, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3339, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3335, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3543, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3433, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3658, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3335, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3245, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3349, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3281, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3393, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3497, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3487, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3178, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3413, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3317, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3341, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.338, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3472, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.319, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3424, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3168, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3354, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3358, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3315, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3196, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3365, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.337, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3411, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3397, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3287, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3287, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3469, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3318, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3655, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3136, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3571, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3024, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3034, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3534, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.314, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3505, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.348, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3241, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3343, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3113, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3621, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3376, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3347, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3272, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.341, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3308, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3334, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3454, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3526, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3186, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.311, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3585, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3349, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.372, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3425, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3284, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3373, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3375, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3406, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3213, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3127, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3417, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3196, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3592, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3365, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3278, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3461, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.337, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3268, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3245, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3329, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3494, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3409, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3375, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3292, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3374, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3541, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.332, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3218, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3316, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3316, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3451, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3081, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2931, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3289, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3452, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3327, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3496, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3323, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3254, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3232, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3394, 'learning_rate': 2e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:323\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 323\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1875\u001b[0m ):\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2781\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2779\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   2780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2781\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1962\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1964\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/phi-2-mlb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72acc6f5c565441b90d2e549e03eded4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3431, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2935, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3622, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3628, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3065, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.327, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2861, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3078, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.257, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3154, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2921, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2642, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2738, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3074, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2767, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2801, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3004, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3332, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2407, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2958, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2823, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3616, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2802, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3611, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3116, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2748, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2687, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2557, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3419, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.288, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2991, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2811, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2891, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2363, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3142, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3194, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.326, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2243, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2744, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2334, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3528, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2617, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2807, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.331, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2946, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2683, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3193, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2541, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3045, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.1749, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2815, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.288, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3375, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3137, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3356, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3134, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3028, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3475, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3072, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2727, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2385, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3346, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2557, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3299, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.319, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3728, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3511, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3628, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.364, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.351, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2803, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3753, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3222, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2991, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2683, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.387, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3354, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3604, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3097, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3667, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2233, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3486, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3208, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2807, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3401, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2699, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3832, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2459, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3144, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.275, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.272, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2973, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3077, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2881, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.375, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3279, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3333, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.304, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2852, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2815, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3152, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.325, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2379, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3023, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2424, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2736, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2773, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3295, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2729, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3903, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2943, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3302, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2988, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3277, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2514, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2727, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2978, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2724, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.334, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3334, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2146, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2679, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3632, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3028, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3062, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3171, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2093, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2343, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3534, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.193, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2306, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3104, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2864, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3292, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3483, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2352, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3094, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3273, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3221, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2858, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2981, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3287, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3108, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3235, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3158, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3193, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2709, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2999, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3081, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3223, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2991, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2968, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3337, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2741, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2941, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2722, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2612, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.297, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3689, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3076, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.4093, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3237, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3146, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3313, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2707, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2515, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2841, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3339, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.319, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3041, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3384, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2576, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2929, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3621, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3262, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3206, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3185, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2892, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3034, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3044, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2001, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2541, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.237, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2645, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2861, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2383, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.339, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3623, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3683, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.302, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.295, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2505, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3435, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3287, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3094, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2788, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3194, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.319, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3628, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3177, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3369, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.319, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3402, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2794, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3818, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3275, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3363, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3542, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2978, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.351, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2984, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3419, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3536, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3264, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2324, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3058, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3328, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3201, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.4068, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3467, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3707, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2602, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.2577, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3152, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 2.263, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.277, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2504, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.349, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3346, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3142, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2392, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3391, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3261, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2977, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.361, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3463, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2912, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2479, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2717, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3185, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3599, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2934, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3748, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2759, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2553, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3508, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2732, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3282, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3529, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.2613, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.23, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.324, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3597, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3351, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 2.3324, 'learning_rate': 2e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/phi-2-mlb/checkpoint-1344\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:323\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 323\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1875\u001b[0m ):\n\u001b[0;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2772\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2772\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2775\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2795\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2794\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2795\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2796\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2797\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\utils\\operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\utils\\operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\utils\\operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\utils\\operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 675 (3 times), autocast_decorator.<locals>.decorate_autocast at line 14 (3 times), convert_outputs_to_fp32.<locals>.forward at line 687 (3 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\utils\\operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\utils\\operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\peft_model.py:1083\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[0;32m   1082\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1084\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1085\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1086\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1087\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1088\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1089\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1090\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1092\u001b[0m     )\n\u001b[0;32m   1094\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\phi-2\\modeling_phi.py:1049\u001b[0m, in \u001b[0;36mPhiForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1046\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1049\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1062\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\phi-2\\modeling_phi.py:919\u001b[0m, in \u001b[0;36mPhiModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    916\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m--> 919\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    929\u001b[0m         hidden_states,\n\u001b[0;32m    930\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    935\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m kwargs))\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reentrant:\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[0;32m    252\u001b[0m         function,\n\u001b[0;32m    253\u001b[0m         preserve,\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    256\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\autograd\\function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m    104\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\phi-2\\modeling_phi.py:679\u001b[0m, in \u001b[0;36mPhiDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[0;32m    669\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    670\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    671\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    675\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    676\u001b[0m )\n\u001b[0;32m    677\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(attn_outputs)\n\u001b[1;32m--> 679\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    680\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_outputs \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states \u001b[38;5;241m+\u001b[39m residual\n\u001b[0;32m    681\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\transformers_modules\\phi-2\\modeling_phi.py:212\u001b[0m, in \u001b[0;36mPhiMLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 212\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(hidden_states)\n\u001b[0;32m    214\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:248\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    245\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m    247\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m--> 248\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[1;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul4Bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\autograd\\function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[1;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_4bit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[0;32m    519\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\functional.py:931\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[1;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[0;32m    929\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_fp16_fp4(get_ptr(\u001b[38;5;28;01mNone\u001b[39;00m), get_ptr(A), get_ptr(absmax), get_ptr(out), ct\u001b[38;5;241m.\u001b[39mc_int(blocksize), ct\u001b[38;5;241m.\u001b[39mc_int(n))\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 931\u001b[0m         \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdequantize_blockwise_fp16_nf4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabsmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quant_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"../models/phi-2-mlb/checkpoint-1344\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:249: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# base model\n",
    "base_path=\"../models/phi-2\"  \n",
    "\n",
    "# adapters: path to folder with adapter_model.safetensors\n",
    "adapter_path=\"../models/phi-2-mlb/checkpoint-1344\" \n",
    "\n",
    "# # where to save merged model\n",
    "# save_to=\"../models/phi-2-mlb/\"       \n",
    "\n",
    "# # Load model and tokenizer\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_path,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "\n",
    "# # Add/set tokens same tokens to base model before merging, like we did before training  \n",
    "# tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "# tokenizer.pad_token = \"<PAD>\"\n",
    "# tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "\n",
    "# base_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# # Set a default Generation configuration: Llama precise\n",
    "# generation_config = GenerationConfig(\n",
    "#     max_new_tokens=100, \n",
    "#     temperature=0.7,\n",
    "#     top_p=0.1,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.18,\n",
    "#     do_sample=True,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# Load LoRA and merge\n",
    "merged_model = PeftModel.from_pretrained(model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# model.save_pretrained(save_to, safe_serialization=True, max_shard_size='4GB')\n",
    "# tokenizer.save_pretrained(save_to)\n",
    "# generation_config.save_pretrained(save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A:\n",
      "\n",
      "You can use a recursive function to parse the JSON and return a dictionary.\n",
      "\n",
      "const data = {\n",
      "  \"result\": {\n",
      "    \"inning\": 1,\n",
      "    \"outs_when_up\": 1,\n",
      "    \"on_1b\": \"\",\n",
      "    \"on_2b\": {\"id\": 435062, \"name\": \"howie kendrick\"},\n",
      "    \"on_3b\": \"\",\n",
      "    \"home_score\": 0,\n",
      "    \"away_score\": 0,\n",
      "    \"pitcher\": {\n",
      "      \"id\": 460024,\n",
      "      \"name\": \"luke hochevar\",\n",
      "    },\n",
      "    \"batter\": {\n",
      "      \"id\": 110029,\n",
      "      \"name\": \"bobby abreu\",\n",
      "    },\n",
      "    \"p_throws\": \"R\",\n",
      "    \"stand\": \"L\",\n",
      "    \"inning_topbot\": \"Top\",\n",
      "  }\n",
      "};\n",
      "\n",
      "function parse(data) {\n",
      "  const result = {\n",
      "    inning: data.inning,\n",
      "    outs_when_up: data.outs_when_up,\n",
      "    on_1b: data.on_1b,\n",
      "    on_2b: {\n",
      "      id: data.on_2b.id,\n",
      "      name: data.on_2b.name,\n",
      "    },\n",
      "    on_3b: data.on_3b,\n",
      "    home_score: data.home_score,\n",
      "    away_score: data.away_score,\n",
      "    pitcher: {\n",
      "      id: data.pitcher.id,\n",
      "      name: data.pitcher.name,\n",
      "    },\n",
      "    batter: {\n",
      "      id: data.batter.id,\n",
      "      name: data.batter.name,\n",
      "    },\n",
      "    p_throws: data.p_throws,\n",
      "    stand: data.stand,\n",
      "    inning_topbot: data.inning_topbot,\n",
      "  };\n",
      "  if (data.pitcher) {\n",
      "    result.pitcher = parse(data.pitcher);\n",
      "  }\n",
      "  if (data.batter) {\n",
      "    result.batter = parse(data.batter);\n",
      "  }\n",
      "  return result;\n",
      "}\n",
      "\n",
      "console.log(parse(data));\n",
      "\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path=\"../models/phi-2-mlb/\"   \n",
    "\n",
    "prompt=\"Instruct: {\\\"input\\\": {\\\"pitcher\\\": {\\\"id\\\": 460024, \\\"name\\\": \\\"luke hochevar\\\"}, \\\"batter\\\": {\\\"id\\\": 110029, \\\"name\\\": \\\"bobby abreu\\\"}, \\\"p_throws\\\": \\\"R\\\", \\\"stand\\\": \\\"L\\\", \\\"inning_topbot\\\": \\\"Top\\\", \\\"inning\\\": 1, \\\"outs_when_up\\\": 1, \\\"on_1b\\\": \\\"\\\", \\\"on_2b\\\": {\\\"id\\\": 435062, \\\"name\\\": \\\"howie kendrick\\\"}, \\\"on_3b\\\": \\\"\\\", \\\"home_score\\\": 0, \\\"away_score\\\": 0}}? \\n\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_path,    \n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path) \n",
    "\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_tokens = merged_model.generate(**input_tokens, max_new_tokens=512)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    output_tokens[0][len(input_tokens[0]):],\n",
    "    skip_special_tokens=True\n",
    "    )               \n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
