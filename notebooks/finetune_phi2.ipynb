{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (0.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: peft in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (2.1.2+cu121)\n",
      "Requirement already satisfied: transformers in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (4.35.2)\n",
      "Requirement already satisfied: tqdm in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (0.26.1)\n",
      "Requirement already satisfied: safetensors in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers->peft) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: trl in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (0.7.10)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from trl) (2.1.2+cu121)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from trl) (4.35.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from trl) (1.26.4)\n",
      "Requirement already satisfied: accelerate in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from trl) (0.26.1)\n",
      "Requirement already satisfied: datasets in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from trl) (2.16.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from trl) (0.7.1)\n",
      "Requirement already satisfied: filelock in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from torch>=1.4.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers>=4.31.0->trl) (4.66.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (1.6.5)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tyro>=0.5.11->trl) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from accelerate->trl) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from datasets->trl) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from datasets->trl) (2.2.0)\n",
      "Requirement already satisfied: xxhash in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from datasets->trl) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from pandas->datasets->trl) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorboard in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (2.15.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (1.60.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tensorboard) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (4.35.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Requirement already satisfied: filelock in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\phi-2\\notebooks\\.venv\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed transformers-4.37.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops\n",
    "%pip install peft\n",
    "%pip install trl\n",
    "%pip install tensorboard\n",
    "%pip install -U transformers\n",
    "# %pip install -U accelerate datasets \n",
    "# %pip install -q https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
    "# %pip install tokenizers==0.15.0\n",
    "# %pip install torch==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install torchaudio==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install torchvision==0.16.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "# %pip install transformers==4.35.2\n",
    "# %pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/danm/.cache/huggingface/datasets/json/default-75cc6199393bf5a9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./2011_2023_event_des.json\", field='data', split='all')\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['train'],\n",
       "        num_rows: 717166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['train'],\n",
       "        num_rows: 79686\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/phi-2-mlb/tokenizer_merged\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "# tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636282df6afe4eb1955de20606d7a983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"../models/phi-2\", \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(60359, 2560)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32, \n",
    "    target_modules = [ \"q_proj\", \"k_proj\", \"v_proj\", \"dense\" ],\n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj','dense','fc1','fc2',]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# dataset-specific parameters\n",
    "bs=8    # batch size for training\n",
    "bs_eval=16    # batch size for evaluation\n",
    "ga_steps=16  # gradient accumulation steps\n",
    "lr=0.00002  # learning rate\n",
    "epochs=1\n",
    "\n",
    "steps_per_epoch=len(dataset[\"train\"])//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/phi-2-mlb\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs_eval,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch//2,    # 2 evals per epoch\n",
    "    save_steps=steps_per_epoch//100,\n",
    "    save_total_limit=3,     # save once per epoch\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",      # val_loss will go nan with paged_adamw_8bit\n",
    "    learning_rate=lr,\n",
    "    group_by_length=False,\n",
    "    bf16=True,        \n",
    "    ddp_find_unused_parameters=False,\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e6bd3840e4b2eada7735521b4bdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/717166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6124440dc08a495caa00af2330a40d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"train\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e157c5a6edfb44dc99dd0cf1629f7460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8374, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.8606, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.7135, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.6986, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.6031, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.6055, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.5103, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.4549, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.4295, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.3531, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.3036, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.2769, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.1592, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.142, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.1154, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.9963, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.0159, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.9626, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.9058, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.8031, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.837, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.7575, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6906, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6105, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.615, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.5404, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4806, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4789, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.3764, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.331, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.3072, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.2952, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.2315, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.1611, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.0753, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.0411, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.9837, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.9694, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.8983, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.8446, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7773, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6138, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5749, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4927, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5261, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4148, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3073, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2987, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1943, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2179, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1363, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.0825, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.07, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.9753, 'learning_rate': 2e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9528, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.8366, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.7526, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.8025, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.7565, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.684, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.6601, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5837, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5416, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4728, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4205, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.3281, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.3345, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2849, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2055, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2384, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.1677, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.1868, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.0833, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.0675, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9923, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9442, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9577, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9161, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8755, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9295, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9127, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8975, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8256, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8085, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8338, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7265, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7724, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7245, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7656, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7234, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6983, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6972, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6039, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6057, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6158, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6228, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5837, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5619, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5824, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5401, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5363, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5132, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5126, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4862, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5202, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4805, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5177, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4673, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4466, 'learning_rate': 2e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4622, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4378, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4544, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4402, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4213, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3582, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4258, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3537, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3851, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.365, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3562, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3772, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3262, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.341, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3162, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3055, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2941, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2623, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2753, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3023, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3024, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.283, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3158, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2578, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2685, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2489, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2427, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2597, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2312, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2415, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2733, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2268, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2002, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1971, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1733, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2375, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2268, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1696, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1581, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1957, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1835, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1853, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1599, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1788, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1731, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1546, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1671, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1163, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1821, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1234, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1174, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1245, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.092, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1361, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1438, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1538, 'learning_rate': 2e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1267, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.145, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1173, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0815, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1049, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1107, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0863, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1001, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1014, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1084, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0647, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0581, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.038, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0561, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1022, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0859, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0389, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0648, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0771, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0824, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0278, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0224, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0338, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0253, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0189, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0326, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0412, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0154, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0467, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0523, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0345, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0151, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9871, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0129, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0237, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0148, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9901, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9986, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9932, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0021, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9867, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.018, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0025, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9252, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9988, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9736, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9839, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9276, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0297, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.979, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9435, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9993, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9877, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9941, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9566, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9897, 'learning_rate': 2e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9589, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9551, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9303, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9766, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.975, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9192, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.93, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9517, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9025, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9212, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9775, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.962, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9339, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.927, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9266, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.936, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9065, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9104, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9132, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9476, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9462, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9382, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9251, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9237, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9174, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9346, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.909, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9162, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9274, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9313, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8488, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8957, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8829, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9095, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.918, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8888, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9075, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8985, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9016, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8734, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.902, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8914, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.897, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8903, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8834, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9439, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8755, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8973, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.883, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8611, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8395, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.889, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8967, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8451, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8596, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8922, 'learning_rate': 2e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.856, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8727, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8636, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8451, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8975, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8592, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8466, 'learning_rate': 2e-05, 'epoch': 0.05}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/phi-2-mlb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(resume_from_checkpoint=\"../models/phi-2-mlb/checkpoint-1344\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:249: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# base model\n",
    "base_path=\"../models/phi-2\"  \n",
    "\n",
    "\n",
    "# adapters: path to folder with adapter_model.safetensors\n",
    "adapter_path=\"../models/phi-2-mlb/\" \n",
    "\n",
    "# # where to save merged model\n",
    "save_to=\"../models/phi-2-mlb/\"       \n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.18,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Load LoRA and merge\n",
    "merged_model = PeftModel.from_pretrained(model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(save_to, safe_serialization=True, max_shard_size='4GB')\n",
    "tokenizer.save_pretrained(save_to)\n",
    "generation_config.save_pretrained(save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path=\"../models/phi-2-mlb/\"   \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prompt=\"Instruct: {\\\"input\\\": {\\\"pitcher\\\": {\\\"id\\\": 460024, \\\"name\\\": \\\"luke hochevar\\\"}, \\\"batter\\\": {\\\"id\\\": 110029, \\\"name\\\": \\\"bobby abreu\\\"}, \\\"p_throws\\\": \\\"R\\\", \\\"stand\\\": \\\"L\\\", \\\"inning_topbot\\\": \\\"Top\\\", \\\"inning\\\": 1, \\\"outs_when_up\\\": 1, \\\"on_1b\\\": \\\"\\\", \\\"on_2b\\\": {\\\"id\\\": 435062, \\\"name\\\": \\\"howie kendrick\\\"}, \\\"on_3b\\\": \\\"\\\", \\\"home_score\\\": 0, \\\"away_score\\\": 0}}? \\n\"\n",
    "\n",
    "\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_tokens = merged_model.generate(**input_tokens, max_new_tokens=512)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    output_tokens[0][len(input_tokens[0]):],\n",
    "    skip_special_tokens=True\n",
    "    )               \n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
