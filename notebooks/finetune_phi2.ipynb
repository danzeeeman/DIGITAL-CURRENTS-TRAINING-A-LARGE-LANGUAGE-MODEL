{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install einops\n",
    "%pip install peft\n",
    "%pip install trl\n",
    "%pip install tensorboard\n",
    "%pip install -U transformers\n",
    "%pip install -U accelerate datasets \n",
    "%pip install -q https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
    "%pip install tokenizers==0.15.0\n",
    "%pip install torch==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install torchaudio==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install torchvision==0.16.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install transformers==4.35.2\n",
    "%pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/danm/.cache/huggingface/datasets/json/default-75cc6199393bf5a9/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./2011_2023_event_des.json\", field='data', split='all')\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['train'],\n",
       "        num_rows: 717166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['train'],\n",
       "        num_rows: 79686\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/phi-2-mlb/tokenizer_merged\", trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636282df6afe4eb1955de20606d7a983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"../models/phi-2\", \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(60359, 2560)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32, \n",
    "    target_modules = [ \"q_proj\", \"k_proj\", \"v_proj\", \"dense\" ],\n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj','dense','fc1','fc2',]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-specific parameters\n",
    "bs=8    # batch size for training\n",
    "bs_eval=16    # batch size for evaluation\n",
    "ga_steps=16  # gradient accumulation steps\n",
    "lr=0.00002  # learning rate\n",
    "epochs=1\n",
    "\n",
    "steps_per_epoch=len(dataset[\"train\"])//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/phi-2-mlb\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs_eval,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch//2,    # 2 evals per epoch\n",
    "    save_steps=steps_per_epoch//100,\n",
    "    save_total_limit=3,     # save once per epoch\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",      # val_loss will go nan with paged_adamw_8bit\n",
    "    learning_rate=lr,\n",
    "    group_by_length=False,\n",
    "    bf16=True,        \n",
    "    ddp_find_unused_parameters=False,\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e6bd3840e4b2eada7735521b4bdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/717166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6124440dc08a495caa00af2330a40d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"train\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"../models/phi-2-mlb/checkpoint-1344\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e157c5a6edfb44dc99dd0cf1629f7460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.8374, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.8606, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.7135, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.6986, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.6031, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.6055, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.5103, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.4549, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.4295, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.3531, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.3036, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.2769, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.1592, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.142, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.1154, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.9963, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 5.0159, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.9626, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.9058, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.8031, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.837, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.7575, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6906, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6105, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.615, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.5404, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4806, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4789, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 4.3764, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.331, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.3072, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.2952, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.2315, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.1611, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.0753, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 4.0411, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.9837, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.9694, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.8983, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.8446, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7773, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6138, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5749, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4927, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5261, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4148, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3073, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2987, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1943, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.2179, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.1363, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.0825, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 3.07, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.9753, 'learning_rate': 2e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9528, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.8366, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.7526, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.8025, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.7565, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.684, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.6601, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5837, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5416, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4728, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4205, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.4418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.3281, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.3345, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2849, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2055, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.2384, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.1677, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.1868, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.0833, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 2.0675, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9923, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9442, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9577, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9161, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8755, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9295, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9127, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 1.8975, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8256, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8085, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.8338, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7265, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7724, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7245, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7656, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7234, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6983, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6972, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6039, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6057, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6158, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.6228, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5837, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5619, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5824, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5401, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5363, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5132, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5126, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4862, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5202, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4805, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.5177, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4673, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4466, 'learning_rate': 2e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4622, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4378, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4544, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4402, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4213, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3582, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.4258, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3537, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3851, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.365, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3562, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3772, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3262, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.341, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3162, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3055, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2941, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2623, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2753, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3023, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3024, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.283, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.3158, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2578, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2685, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2489, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2427, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2597, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 1.2312, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2415, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2733, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2268, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2002, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1971, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1733, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2375, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.2268, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1696, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1581, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1957, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1835, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1853, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1599, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1788, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1731, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1546, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1671, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1163, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1821, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1234, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1174, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1245, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.092, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1361, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1438, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1538, 'learning_rate': 2e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1267, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.145, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1173, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0815, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1049, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1107, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0863, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1001, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1014, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1084, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0647, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0581, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.038, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0561, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1022, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0859, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0389, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0648, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0771, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0824, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0278, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0224, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0338, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0253, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0189, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0326, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0412, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0154, 'learning_rate': 2e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0467, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0523, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0345, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0151, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9871, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0129, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0237, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0148, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9901, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9986, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9932, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0021, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9867, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.018, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0025, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9252, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9988, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9736, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9839, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9276, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0297, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.979, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9435, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9993, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9877, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9941, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9566, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9897, 'learning_rate': 2e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9589, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9551, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9303, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9766, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.975, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9192, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.93, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9517, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9025, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9212, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9775, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.962, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9339, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.927, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9266, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.936, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9065, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9104, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9132, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9476, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9462, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9382, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9251, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9237, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9174, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9346, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.909, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9162, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9274, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9313, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8488, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8957, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8829, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9095, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.918, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8888, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9075, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8985, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9016, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8734, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.902, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8914, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.897, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8903, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8834, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9439, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8755, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8973, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.883, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8611, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8395, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.889, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8967, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8451, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8596, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8922, 'learning_rate': 2e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.856, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8727, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8636, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8451, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8975, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8592, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8466, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8351, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8695, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8195, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.847, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8406, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8308, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8007, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8356, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8633, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8394, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8238, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8177, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8274, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8101, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8382, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8287, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8333, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.797, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8267, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8187, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.808, 'learning_rate': 2e-05, 'epoch': 0.05}\n",
      "{'loss': 0.8258, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7995, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8296, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7831, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7987, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8338, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8314, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8239, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8005, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7944, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8102, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.823, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8176, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8049, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8084, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8194, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.787, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8094, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7659, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7673, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7793, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8193, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7721, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.8335, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7724, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7881, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7613, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7923, 'learning_rate': 2e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7786, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7495, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7838, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7744, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7536, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7531, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7794, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7752, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7833, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7645, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.795, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7476, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7693, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.758, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7616, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7766, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7635, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7928, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.789, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7643, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.754, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7315, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7309, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7354, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7621, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7701, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7578, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.726, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 0.7451, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.732, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7392, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7493, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7265, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.758, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7573, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7317, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7139, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7178, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7277, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7806, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7404, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7131, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7296, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7115, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7352, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7337, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7738, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7139, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7109, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7239, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7075, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6967, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7064, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7008, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6972, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7057, 'learning_rate': 2e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7244, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7183, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7128, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6971, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7132, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7226, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7089, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7144, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7194, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6908, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7305, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7166, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.677, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7051, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6897, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6785, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.711, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7142, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7137, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6978, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7198, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.706, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6918, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6865, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.703, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7134, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7242, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.7005, 'learning_rate': 2e-05, 'epoch': 0.07}\n",
      "{'loss': 0.6928, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6899, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6708, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.664, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.675, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6901, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6911, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6791, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6711, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6567, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6768, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.675, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6719, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.7066, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6939, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6684, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6838, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6534, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6567, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6696, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.655, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6934, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6513, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.631, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6629, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6928, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6736, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6907, 'learning_rate': 2e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6775, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6652, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6676, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.653, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6736, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6543, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6757, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.681, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6718, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6726, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6533, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6416, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6452, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6493, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6761, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6532, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6741, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6718, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6509, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6649, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6764, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6509, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6478, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6622, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6554, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.65, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6391, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.653, 'learning_rate': 2e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6821, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6444, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6501, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6319, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6171, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6399, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6424, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6291, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6529, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6169, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6045, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6213, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6471, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6137, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6652, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6107, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6246, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6547, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6468, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6074, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6121, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6062, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6377, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.629, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6197, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6478, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6051, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6468, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6158, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6228, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6195, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5928, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6064, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6037, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6219, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6129, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6127, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6196, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6255, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6234, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6124, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6329, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6085, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6031, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6067, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.592, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6241, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6222, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6325, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5915, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5805, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6004, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6035, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6107, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.599, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.6147, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 0.5941, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5668, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6066, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5908, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5834, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5961, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5858, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5853, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5859, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5856, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5784, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5761, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6124, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.592, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.584, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5868, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6003, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6163, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5665, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6081, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5732, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5874, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5786, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5786, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5848, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5836, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5882, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5846, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5742, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5852, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5785, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5717, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5828, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6077, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5826, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.588, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5851, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5691, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.6247, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5827, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5691, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5898, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5656, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.55, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5852, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5591, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5733, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5488, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5639, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5694, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5921, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5715, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5657, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5659, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5585, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5625, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 0.5753, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5498, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5434, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5759, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.551, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5576, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5561, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.564, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5423, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5399, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5477, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5471, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5789, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5851, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5516, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5622, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5583, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5389, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5779, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5596, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5357, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5442, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.564, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5528, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5592, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5387, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5426, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5688, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5435, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5584, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5351, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5639, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5903, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5437, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5362, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5493, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5726, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5341, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5497, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.552, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5487, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5326, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5241, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.534, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.547, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5601, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5405, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5657, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.542, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5299, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5471, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5215, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5308, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5111, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5327, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5225, 'learning_rate': 2e-05, 'epoch': 0.11}\n",
      "{'loss': 0.5411, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5273, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5151, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5209, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.51, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5316, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5387, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5226, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5449, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5476, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5427, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5192, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5256, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.534, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5319, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.537, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5324, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5087, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5412, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5227, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5136, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5149, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5202, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5265, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5079, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5154, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5596, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5034, 'learning_rate': 2e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5089, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5169, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5088, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5064, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4918, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.52, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5204, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5226, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5103, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5205, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5022, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.517, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5288, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5089, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5021, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.502, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4903, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5214, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.499, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.523, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5139, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5096, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4959, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5008, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4884, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.4893, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.5094, 'learning_rate': 2e-05, 'epoch': 0.12}\n",
      "{'loss': 0.501, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4837, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4915, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4892, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5166, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4748, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4818, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4763, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4767, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4979, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4849, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4998, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5137, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4855, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5014, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4971, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4764, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4796, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5203, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5042, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.515, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.507, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5144, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4833, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4913, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4775, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4952, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5216, 'learning_rate': 2e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5226, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4747, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4755, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4808, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4844, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4723, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4727, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5087, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5189, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4968, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5082, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4986, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4808, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5089, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4912, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.508, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4541, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4867, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4901, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4935, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4769, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4815, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5148, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4887, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4721, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4947, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4928, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.4898, 'learning_rate': 2e-05, 'epoch': 0.13}\n",
      "{'loss': 0.5003, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.5062, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4817, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.491, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4742, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.5117, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.485, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4802, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4716, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.498, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4899, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4355, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4676, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4662, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4874, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4753, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4784, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4642, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4629, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4731, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4558, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4638, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4772, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4863, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4821, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4666, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4734, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.46, 'learning_rate': 2e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4659, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4427, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4723, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4778, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4896, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4685, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4807, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4658, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4779, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4623, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4774, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4795, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4768, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4789, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.448, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4958, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4667, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4919, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4651, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4664, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4718, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4358, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4649, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.465, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4677, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4498, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4531, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4811, 'learning_rate': 2e-05, 'epoch': 0.14}\n",
      "{'loss': 0.4599, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4409, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4669, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4468, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4476, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4375, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4578, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4592, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4556, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.448, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4828, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4593, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4655, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4304, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4855, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4639, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4263, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4647, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4393, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4555, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.441, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4495, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4948, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4414, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4541, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4396, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4473, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.445, 'learning_rate': 2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4479, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4735, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4317, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4569, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4456, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4586, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4636, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4439, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4524, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4432, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4579, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4502, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4483, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4463, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4507, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4471, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4567, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4353, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4288, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4478, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4403, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.442, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4339, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4636, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4512, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4769, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4316, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4477, 'learning_rate': 2e-05, 'epoch': 0.15}\n",
      "{'loss': 0.4318, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4493, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4459, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4392, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4832, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4554, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4209, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4472, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4555, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4522, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4517, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4426, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4735, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4602, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4449, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4298, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4457, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4638, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4346, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4399, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4415, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.443, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.428, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4243, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4716, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4348, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4271, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4233, 'learning_rate': 2e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4248, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4287, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.424, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.463, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4282, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4176, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4407, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.432, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4257, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4524, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4379, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4427, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4464, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4183, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4163, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4361, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4118, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4353, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4178, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.402, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.42, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4347, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4194, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4409, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4247, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4101, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.429, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4329, 'learning_rate': 2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.4386, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4505, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4232, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4341, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4435, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4299, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4331, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4144, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4101, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4143, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4249, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4254, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4035, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4326, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4285, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4077, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4317, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4356, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4307, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4101, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4112, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4217, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4007, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.444, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4073, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4452, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4056, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4249, 'learning_rate': 2e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.436, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4168, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4246, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4344, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.421, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4274, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.435, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.397, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.426, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4342, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.3998, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.431, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4081, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4146, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4105, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.3989, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4242, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4339, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4199, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.405, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4201, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4428, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4143, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4286, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4202, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4131, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4058, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4301, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 0.4147, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4078, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3992, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4052, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4326, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4138, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4248, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.415, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.413, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3954, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4242, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.409, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.411, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4263, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4085, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4072, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3973, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4248, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4155, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4255, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4178, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.411, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3929, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.424, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4431, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.414, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4236, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4147, 'learning_rate': 2e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4096, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4162, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.407, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4191, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3906, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4083, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4267, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4096, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4115, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4244, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3909, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4014, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.421, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3899, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3948, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3999, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3946, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4058, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4377, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3979, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4077, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4057, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4162, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4042, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.403, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3909, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3841, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.4084, 'learning_rate': 2e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3981, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3903, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4125, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4131, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4144, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4151, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.426, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3928, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4084, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4017, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3963, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3968, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4074, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.406, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3798, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4087, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4013, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3996, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4209, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4002, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3993, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3917, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.402, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4104, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3941, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4075, 'learning_rate': 2e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4247, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3913, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3797, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4164, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4001, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4076, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3985, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4028, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4118, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3827, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3913, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4139, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4053, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4011, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4098, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4063, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3982, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4108, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4061, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3846, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4183, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3926, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3813, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4113, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3908, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3974, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3965, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.19}\n",
      "{'loss': 0.4016, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4113, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4087, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4035, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.391, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3894, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4034, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.383, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3902, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4059, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4002, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3867, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3943, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3924, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3951, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3815, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.389, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3918, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.398, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3857, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4014, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3951, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3924, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3899, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3947, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4041, 'learning_rate': 2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4008, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3954, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3815, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3796, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4011, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3897, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3908, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3932, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3839, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.379, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3785, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3801, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3731, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4143, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3971, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3893, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3853, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3728, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.424, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4021, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.4, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3935, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3876, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3988, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3938, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3851, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3773, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.3832, 'learning_rate': 2e-05, 'epoch': 0.2}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.374, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3966, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3959, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3835, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3795, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3843, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3915, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3893, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3847, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3861, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4011, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3846, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.381, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3621, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3846, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.407, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3797, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3805, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.377, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3826, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4102, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3974, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3864, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3875, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3842, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3796, 'learning_rate': 2e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3848, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3805, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3831, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3839, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.376, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3685, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3834, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3911, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3749, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3743, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3723, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3791, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3746, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3769, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3827, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3771, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3892, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3888, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3868, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.372, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3764, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3582, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3833, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.367, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3882, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3911, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3778, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.3592, 'learning_rate': 2e-05, 'epoch': 0.21}\n",
      "{'loss': 0.375, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3713, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3875, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3952, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3863, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.414, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.396, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.395, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3631, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3701, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3777, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3808, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3746, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3766, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3923, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3702, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4045, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3835, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3684, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3961, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4048, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3737, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3688, 'learning_rate': 2e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3749, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3865, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3871, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.357, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3772, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3716, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3672, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3536, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3847, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3852, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3848, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3934, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3639, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.4069, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3842, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3817, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3912, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3591, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3921, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3572, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3803, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3884, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3799, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3747, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3772, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3792, 'learning_rate': 2e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3593, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3946, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3661, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3752, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3852, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3769, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.403, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.401, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3754, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3762, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3802, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3689, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3828, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3424, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3819, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3777, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3684, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3754, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3692, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3784, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3627, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3542, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3821, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3739, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.347, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3789, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3651, 'learning_rate': 2e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3684, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3624, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3697, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3561, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3779, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3634, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3764, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3623, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3783, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3645, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3833, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3703, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.364, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3691, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3771, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3845, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3542, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3685, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3938, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3876, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3583, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3757, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3643, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.383, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3756, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3632, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3617, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3807, 'learning_rate': 2e-05, 'epoch': 0.23}\n",
      "{'loss': 0.3508, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.4019, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3522, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3632, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3718, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3623, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3825, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3442, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3648, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3625, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3853, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3689, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3714, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3661, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3608, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3763, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3814, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.361, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3605, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.367, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3679, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.369, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3695, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3814, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3663, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3739, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3882, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3576, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3694, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3729, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3754, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3596, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3567, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.36, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3612, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3769, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3525, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3514, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3631, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3713, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3644, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3575, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3555, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3684, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3769, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3614, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3586, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.354, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3518, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3828, 'learning_rate': 2e-05, 'epoch': 0.24}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3542, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.383, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3658, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3733, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3655, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3687, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3913, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3761, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3569, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3678, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3626, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.362, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3707, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3527, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3794, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3504, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3729, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3734, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.391, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3742, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3582, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3608, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3942, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3705, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3647, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3541, 'learning_rate': 2e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.358, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3678, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3532, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3737, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3606, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3469, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3394, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3586, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3649, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3751, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3683, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3609, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3613, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3626, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3724, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3607, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3575, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3681, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3724, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3741, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3727, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3585, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3642, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3671, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3356, 'learning_rate': 2e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3568, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3537, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3524, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3621, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3548, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3656, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3691, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3635, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3575, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3628, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3941, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3698, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3528, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3547, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3774, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3683, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3649, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3454, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3696, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3629, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3575, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3438, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3583, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3464, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3705, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3434, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3656, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3558, 'learning_rate': 2e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3515, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3636, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.385, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3481, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3571, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3679, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3624, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3623, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3676, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3573, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3675, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.373, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3531, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.36, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3596, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3626, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.352, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.355, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3528, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.365, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3442, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3677, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3588, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.358, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3695, 'learning_rate': 2e-05, 'epoch': 0.26}\n",
      "{'loss': 0.3708, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3773, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3496, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3502, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3579, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3617, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3421, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3877, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3468, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3475, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3651, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3648, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3557, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3522, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3627, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3486, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3512, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.367, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3429, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3448, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3534, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3562, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3449, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3615, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3547, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3707, 'learning_rate': 2e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3395, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3521, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3509, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3623, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3572, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3663, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3442, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3471, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3447, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3617, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3601, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.369, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.368, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3485, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3557, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3603, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3512, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3734, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3452, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3562, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3564, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3447, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3539, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3543, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3502, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3661, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3425, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3471, 'learning_rate': 2e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3553, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3334, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3803, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3627, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3495, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3654, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3565, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3654, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3478, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3452, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3564, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3534, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3403, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3665, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3561, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3478, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.353, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3653, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3544, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3562, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3675, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3627, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3599, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.336, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3553, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3644, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3413, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3594, 'learning_rate': 2e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3448, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3579, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3517, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3491, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3572, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3441, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3542, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3559, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3434, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3298, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3538, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.345, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3303, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3643, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3428, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3407, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3479, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3468, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3445, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3405, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3514, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3744, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.355, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3586, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3421, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3492, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3464, 'learning_rate': 2e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3593, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3816, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3432, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3486, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.362, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3528, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3507, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3435, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3783, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3519, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3402, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3652, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3546, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3589, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3667, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3518, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.357, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.354, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3404, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3482, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.35, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3572, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3566, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3471, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3521, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3445, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3551, 'learning_rate': 2e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3524, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3368, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3389, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3598, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.346, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3375, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3551, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3228, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3613, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3518, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3523, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3357, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3374, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3293, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3408, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3439, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3505, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3379, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3363, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.345, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3377, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3485, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3324, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3471, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3487, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.347, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3364, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3467, 'learning_rate': 2e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3512, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3537, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3281, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3463, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3491, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3531, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.344, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.352, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3468, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3471, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3706, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3317, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3479, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3742, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3354, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.339, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3559, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3392, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3571, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3594, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3368, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3597, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.339, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3602, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3505, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3478, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3549, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3404, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3553, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3468, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3449, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3454, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3422, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3256, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3483, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3438, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3421, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3284, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3332, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3386, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3524, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3456, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3487, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3631, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3319, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3388, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3385, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.348, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.35, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3381, 'learning_rate': 2e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3433, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3416, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3297, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3513, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3311, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3337, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3477, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3378, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3494, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3372, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3357, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3524, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3461, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3426, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3476, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3403, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.344, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3388, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3321, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3623, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3332, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3285, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3371, 'learning_rate': 2e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3331, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3397, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3284, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3251, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3349, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3345, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3466, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3391, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3673, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3311, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3401, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3559, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.341, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3591, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3423, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3448, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3301, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3489, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3532, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3462, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3335, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3345, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3373, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3524, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3408, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3268, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3592, 'learning_rate': 2e-05, 'epoch': 0.31}\n",
      "{'loss': 0.335, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3203, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3402, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.348, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3539, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3404, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3262, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3486, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3257, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3664, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3319, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3513, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3304, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3435, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3524, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3432, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3437, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.356, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3358, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3244, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3405, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3441, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3421, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3291, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3197, 'learning_rate': 2e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3456, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3508, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3667, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.35, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3291, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3439, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3686, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3399, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3406, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3632, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3296, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3386, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.343, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.338, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3508, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.342, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3316, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3525, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.337, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3423, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3305, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3334, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3417, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3557, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3498, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3342, 'learning_rate': 2e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3323, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3282, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3483, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3442, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3498, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3295, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3338, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3363, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3346, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3393, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3412, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3373, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3189, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3218, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3557, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3301, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3305, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3451, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3424, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3318, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3375, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3453, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3338, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3353, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.347, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3389, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3373, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3481, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3361, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.329, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3523, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3307, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3429, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3633, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3436, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.346, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.342, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3519, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3518, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3219, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3379, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3326, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3408, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3428, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3424, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3339, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3455, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3406, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3298, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3366, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3454, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3288, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3235, 'learning_rate': 2e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3481, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3244, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.338, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3504, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3419, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3207, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3374, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3508, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3472, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3488, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3432, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3406, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.339, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3392, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.327, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3524, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.347, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3449, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3523, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3423, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3248, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3311, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3323, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3275, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3367, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3329, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3522, 'learning_rate': 2e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3281, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.331, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3357, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3279, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.33, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3351, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3198, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3366, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3463, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3312, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.33, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3331, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3324, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.335, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3338, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3566, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3357, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3379, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3387, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3445, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3386, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3277, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3322, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3325, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.314, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3362, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3365, 'learning_rate': 2e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3423, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3304, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3298, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3383, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3387, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3269, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3371, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3303, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3376, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3193, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3289, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3393, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3268, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3343, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3445, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3455, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.33, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3366, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3289, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3278, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3497, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3194, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3283, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.332, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3285, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3276, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3337, 'learning_rate': 2e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3177, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3462, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3327, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3436, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3336, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3295, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3256, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3365, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3467, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3241, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3426, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3365, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3304, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3199, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3322, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3212, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3319, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3294, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3388, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3315, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.339, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3374, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3512, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.337, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3328, 'learning_rate': 2e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3238, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3295, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3396, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3425, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3348, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3228, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3285, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3346, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3456, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3513, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3242, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3216, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3296, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3302, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3277, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3335, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3205, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3279, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3273, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.346, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3299, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3406, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3307, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3338, 'learning_rate': 2e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3484, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3278, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3346, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3291, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3271, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.336, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.334, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3309, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.337, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3301, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3277, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3431, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3419, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3216, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3235, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3312, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3409, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.341, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3253, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3299, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3269, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3338, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3386, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3263, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3283, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3357, 'learning_rate': 2e-05, 'epoch': 0.36}\n",
      "{'loss': 0.3354, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.333, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3261, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3285, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3365, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.34, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3371, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3212, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3364, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3372, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.327, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3239, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3323, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3589, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3242, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3232, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.318, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3202, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3132, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3223, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3362, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3345, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3387, 'learning_rate': 2e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3283, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3474, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3142, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3317, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3375, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3211, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3356, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.333, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3313, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3145, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3173, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3323, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3381, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3088, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3281, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3326, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3366, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3334, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3271, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3343, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3235, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3352, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3345, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3316, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3318, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3385, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3336, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3186, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3248, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3406, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3279, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.328, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3182, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3223, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3415, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3411, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3271, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3247, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3255, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3168, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3301, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3494, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3326, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3293, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3324, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3422, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3196, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3316, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.318, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3293, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3342, 'learning_rate': 2e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3403, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3265, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3307, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3184, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3061, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.341, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3238, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3224, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3273, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3386, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3348, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3362, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3179, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3411, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3267, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3167, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3298, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3206, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3193, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3213, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3169, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3393, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3322, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3165, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3129, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3504, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3271, 'learning_rate': 2e-05, 'epoch': 0.38}\n",
      "{'loss': 0.3326, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3119, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3103, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3301, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.336, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3185, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3334, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3444, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3245, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.327, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3135, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3475, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3399, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3361, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3236, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3187, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.33, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.326, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3221, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3345, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3309, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3244, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3369, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3161, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.35, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3264, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3372, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3243, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3409, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3282, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3238, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3429, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3312, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3279, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3364, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3221, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3221, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.314, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3172, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3401, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3244, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.316, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3337, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3212, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3389, 'learning_rate': 2e-05, 'epoch': 0.39}\n",
      "{'loss': 0.3206, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3143, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3298, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3385, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.311, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2989, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3292, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3109, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3315, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3128, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3304, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3282, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3253, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3292, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3231, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3098, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3451, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3211, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3197, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3169, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3285, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3219, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3223, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3227, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3288, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3242, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3274, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3193, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3366, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3152, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3143, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3261, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3269, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3346, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3289, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3239, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3307, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3272, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3247, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3167, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.329, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3192, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3332, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3286, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3175, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3339, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3249, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3056, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3224, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3263, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3461, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3013, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3352, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3268, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.348, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3196, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3233, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3236, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3275, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.343, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3265, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3111, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3243, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3187, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3208, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3251, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3333, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3317, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3426, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3108, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3268, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3236, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.323, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3191, 'learning_rate': 2e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.331, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3372, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3066, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3198, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3197, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3085, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3173, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.338, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3302, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3184, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3134, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3149, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.319, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3288, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3077, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3037, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3286, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3246, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3364, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3131, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.339, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3197, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3231, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3277, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3348, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3201, 'learning_rate': 2e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3349, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3265, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3252, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3162, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3431, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.334, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3226, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3068, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.32, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3292, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3162, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.315, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3226, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3288, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3111, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3244, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3216, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3177, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3274, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3319, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3222, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3094, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3267, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3199, 'learning_rate': 2e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.321, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3258, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3198, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3172, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3317, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.306, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3158, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3224, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.311, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3053, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3436, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3267, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3236, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3308, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3264, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3184, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3151, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3236, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3399, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3252, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3282, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3151, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3369, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3256, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.32, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3172, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3217, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3168, 'learning_rate': 2e-05, 'epoch': 0.42}\n",
      "{'loss': 0.319, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3301, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3181, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3384, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3136, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3191, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3221, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3298, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3371, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3231, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3158, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3232, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3206, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.322, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3245, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3179, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3274, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3268, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3094, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3181, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3278, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3132, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3241, 'learning_rate': 2e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3135, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3209, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3128, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3322, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3228, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3299, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3171, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3096, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3395, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3215, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3142, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3147, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3073, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3161, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3196, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2997, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3266, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3254, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3334, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3136, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.315, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3164, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3047, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3153, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.319, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.314, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3279, 'learning_rate': 2e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3206, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3302, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3174, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3269, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.314, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3215, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3213, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3285, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3263, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3244, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3094, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3287, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3266, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3341, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3206, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3095, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3286, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3248, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3319, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3213, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3161, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3295, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3148, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3177, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3225, 'learning_rate': 2e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3263, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3169, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3415, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3215, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3119, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3239, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3188, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3338, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3339, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3148, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3185, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3182, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3176, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3227, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3247, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3167, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.301, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3135, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3262, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3084, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3172, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3345, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3202, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.32, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3136, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3336, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3087, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3221, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3147, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3077, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3122, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3186, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3261, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3121, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3201, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3113, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3175, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3315, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3165, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3209, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3312, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3118, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3232, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3201, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3348, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3096, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3068, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3168, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3068, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.319, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3176, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3299, 'learning_rate': 2e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3329, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3138, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3233, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3124, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3205, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3183, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2939, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3048, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3212, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3203, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3276, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3094, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.321, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3274, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3207, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3158, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3219, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3073, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3209, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3097, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3124, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3249, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3162, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3318, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3089, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.303, 'learning_rate': 2e-05, 'epoch': 0.45}\n",
      "{'loss': 0.3395, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.31, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3249, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3097, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3119, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3274, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3154, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3264, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3127, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3113, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3348, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3339, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3066, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.324, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.309, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3199, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3256, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3278, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.311, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3189, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3252, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3201, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3178, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3349, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3457, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3113, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3071, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3172, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3367, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3223, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.322, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3215, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2834, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3219, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3346, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3222, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3226, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3217, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3177, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3212, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.316, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3093, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.307, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2962, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3189, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.334, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3041, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3243, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3052, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3032, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3206, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3255, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3171, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3136, 'learning_rate': 2e-05, 'epoch': 0.46}\n",
      "{'loss': 0.3147, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3104, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3123, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3188, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3086, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3064, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3285, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3374, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3181, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3262, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3196, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.316, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3343, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3126, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3074, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3146, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3105, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3137, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3129, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.307, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3231, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3141, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3218, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3108, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3151, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3263, 'learning_rate': 2e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3234, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3108, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3138, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3071, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3115, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3106, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.323, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.308, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.319, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.311, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3162, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.315, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3317, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.317, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3325, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3287, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3198, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3104, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3156, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3142, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3121, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3204, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3194, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3262, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3147, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3081, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3091, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3229, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3168, 'learning_rate': 2e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3103, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3453, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3216, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3179, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3054, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3112, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3173, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3191, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3093, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3226, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3138, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2993, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3073, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3354, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3058, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3104, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3186, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3129, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3151, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3038, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3167, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3105, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3198, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3306, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3068, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3218, 'learning_rate': 2e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3083, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3127, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3116, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3131, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3299, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.305, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3132, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3172, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3054, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.304, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.329, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3188, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3088, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3178, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3151, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3175, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3289, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.313, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3196, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3109, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3192, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3261, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3229, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3219, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3145, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3094, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3035, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3046, 'learning_rate': 2e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3163, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3105, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3054, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3033, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3292, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3071, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3198, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.316, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3264, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3174, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3099, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3166, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3063, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3113, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3233, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3108, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3214, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3098, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3259, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3268, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3227, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3153, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.328, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3344, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2945, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3135, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.319, 'learning_rate': 2e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3286, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3311, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3184, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3121, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.32, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3103, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3122, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3156, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3144, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3187, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3172, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3113, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3179, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3073, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.309, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3206, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3107, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3159, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3199, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3185, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3147, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3236, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3263, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3121, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3177, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3258, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.315, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3261, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3047, 'learning_rate': 2e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3119, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3291, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3151, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3075, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3115, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.32, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3156, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3139, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.321, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3228, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3233, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3056, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3242, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3115, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3177, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3139, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3167, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3237, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2998, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.313, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3187, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.325, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3283, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3301, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3187, 'learning_rate': 2e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2947, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3156, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e80e4fafe74ec1b4d610f0a7798fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4981 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/phi-2-mlb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:249: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# base model\n",
    "base_path=\"../models/phi-2\"  \n",
    "\n",
    "\n",
    "# adapters: path to folder with adapter_model.safetensors\n",
    "adapter_path=\"../models/phi-2-mlb/\" \n",
    "\n",
    "# # where to save merged model\n",
    "save_to=\"../models/phi-2-mlb/\"       \n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.18,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Load LoRA and merge\n",
    "merged_model = PeftModel.from_pretrained(model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_model.save_pretrained(save_to, safe_serialization=True, max_shard_size='4GB')\n",
    "tokenizer.save_pretrained(save_to)\n",
    "generation_config.save_pretrained(save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path=\"../models/phi-2-mlb/\"   \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prompt= \"Instruct: what is the outcome of {\\\"input\\\": {\\\"pitcher\\\": {\\\"id\\\": 573204, \\\"name\\\": \\\"caleb thielbar\\\"}, \\\"batter\\\": {\\\"id\\\": 488726, \\\"name\\\": \\\"michael brantley\\\"}}}? \\n\"\n",
    "\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_tokens = model.generate(**input_tokens, max_new_tokens=128)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    output_tokens[0][len(input_tokens[0]):],\n",
    "    skip_special_tokens=True\n",
    "    )               \n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
