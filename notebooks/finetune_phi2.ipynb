{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -eft (c:\\users\\danm\\downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install einops\n",
    "%pip install peft\n",
    "%pip install trl\n",
    "%pip install tensorboard\n",
    "%pip install -q -U https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/danm/.cache/huggingface/datasets/json/default-9ad94e224e55e78c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daf67d2565046f0b82a37a6db339f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb01968d34a74bff9460bbab35fdf69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9826f6e99a4744929a19ce3b68ec7794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/danm/.cache/huggingface/datasets/json/default-9ad94e224e55e78c/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./train.json\", field='data', split='all')\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 717166\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 79686\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445240beb8144b36aaf67359b4df607d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"../models/phi-2\", \n",
    "        quantization_config=bnb_config, \n",
    "        device_map = 'auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True,\n",
    "    )\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True) \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32, \n",
    "    target_modules = [ \"q_proj\", \"k_proj\", \"v_proj\", \"dense\" ],\n",
    "    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj','dense','fc1','fc2',]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# dataset-specific parameters\n",
    "bs=1     # batch size for training\n",
    "bs_eval=16    # batch size for evaluation\n",
    "ga_steps=16  # gradient accumulation steps\n",
    "lr=0.00002  # learning rate\n",
    "epochs=2\n",
    "\n",
    "steps_per_epoch=len(dataset[\"train\"])//(bs*ga_steps)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/phi-2-mlb\",\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs_eval,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=steps_per_epoch//2,    # 2 evals per epoch\n",
    "    save_steps=steps_per_epoch//100,\n",
    "    save_total_limit=3,     # save once per epoch\n",
    "    gradient_accumulation_steps=ga_steps,\n",
    "    num_train_epochs=epochs,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_32bit\",      # val_loss will go nan with paged_adamw_8bit\n",
    "    learning_rate=lr,\n",
    "    group_by_length=False,\n",
    "    bf16=True,        \n",
    "    ddp_find_unused_parameters=False,\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\danm\\.cache\\huggingface\\datasets\\json\\default-9ad94e224e55e78c\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-7fe9337e62cd197f.arrow\n",
      "Loading cached processed dataset at C:\\Users\\danm\\.cache\\huggingface\\datasets\\json\\default-9ad94e224e55e78c\\0.0.0\\0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\\cache-d37a561bab1eb45d.arrow\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844aa585f83e4e7ba4851daccfac09b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89644 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8227, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7298, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7161, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7308, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7457, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.7392, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6812, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6583, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6329, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5237, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.6116, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5893, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5667, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5725, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5247, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.5128, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4649, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4071, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4558, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4403, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3882, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.4108, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3063, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3012, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.3227, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.2909, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.2996, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.226, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1915, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.183, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1882, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.2041, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1628, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1568, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.1538, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0559, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0581, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0751, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0415, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0249, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.0458, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9914, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.987, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 1.023, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9193, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9509, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9229, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9167, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.9192, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8298, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8145, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8163, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7722, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8246, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7722, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.8207, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7817, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7921, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6831, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7704, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7049, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7258, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.7418, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6973, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6739, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6819, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6358, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.651, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6684, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6771, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6156, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6228, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6455, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.645, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5795, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6139, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6158, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.639, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6615, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6148, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5556, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5966, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5661, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6067, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.596, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5679, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6296, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6247, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5694, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5526, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5772, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5581, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6225, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5676, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5312, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5464, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5389, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.579, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5222, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4668, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6003, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.55, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4712, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5489, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5391, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5489, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5364, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5623, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4937, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.546, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5091, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5223, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5554, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5042, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5563, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5352, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4772, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5144, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4877, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5237, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5034, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4663, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5323, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5568, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5079, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4939, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5235, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5114, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4936, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5251, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5047, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5028, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5191, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5371, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5079, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4969, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4777, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4786, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4819, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5019, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4804, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4893, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5008, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4804, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5087, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5142, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4992, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.472, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4783, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4863, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4963, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4804, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4983, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4438, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.449, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4728, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4603, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5064, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4893, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4467, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5217, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.493, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5076, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4699, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4446, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5191, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4673, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5011, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5047, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4742, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4984, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4424, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4887, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4575, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.444, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4835, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4732, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4637, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4762, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.5245, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4638, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4825, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4361, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4372, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4745, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4672, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4556, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4921, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4429, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4437, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4676, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4291, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4796, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4797, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4609, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4454, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4864, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4788, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4569, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4465, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4428, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.486, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4423, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4496, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4134, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4278, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4247, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4513, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4745, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4828, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4431, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4884, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4757, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4848, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4433, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4616, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4563, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4623, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4416, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4705, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4434, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4357, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4454, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4397, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
      "{'loss': 0.4802, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4258, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4542, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4445, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.456, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.467, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4714, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4141, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4432, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4256, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4389, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4318, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4626, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4246, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4508, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.455, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4312, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4742, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4333, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4384, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4357, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.448, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.447, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4433, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4383, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4316, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4616, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4307, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4048, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.395, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4562, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4316, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4232, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4527, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4631, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4214, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4385, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4629, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4397, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4241, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4299, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4532, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4525, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4245, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4215, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4224, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4511, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4303, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.422, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4215, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4376, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4228, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.454, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4103, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4259, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4123, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4509, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3892, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4159, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.459, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4344, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.418, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.42, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3777, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4188, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4188, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4127, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4413, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4485, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4011, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4429, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4157, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.405, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.429, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4152, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4171, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4424, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4125, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.413, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4622, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4251, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4088, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.424, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3998, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4441, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4002, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4154, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4278, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4388, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4212, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4283, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4463, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4075, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4336, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4318, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4504, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4156, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4302, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4015, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.466, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.409, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4277, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4448, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4114, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4366, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3991, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4321, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.43, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4146, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4343, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4151, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4164, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4204, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4462, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4225, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4156, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4436, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4149, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4136, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4255, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4357, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4249, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4351, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4232, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.433, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.401, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4144, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3873, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4116, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4052, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4275, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4026, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4171, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4262, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4295, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4643, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.425, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4479, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4139, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4209, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4168, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4051, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3736, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4147, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4316, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4345, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4056, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3818, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4261, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4135, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4488, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4093, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.409, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3979, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4371, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4118, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.436, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4016, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4314, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4427, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4291, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4034, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3988, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3899, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3959, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3993, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3742, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3799, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.401, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4085, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.392, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4473, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4181, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4389, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4307, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4053, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4037, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3934, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3925, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4079, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4069, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3999, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3725, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4214, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.394, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.388, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4059, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4129, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3992, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4189, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4244, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4146, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3591, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.407, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4289, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3653, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3859, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4374, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3965, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3568, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4032, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3696, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3939, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3898, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4188, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3987, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3761, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3726, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4425, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4068, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3919, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3817, 'learning_rate': 2e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\peft\\utils\\save_and_load.py:148: UserWarning: Could not find a config file in ../models/phi-2 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4009, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.373, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.405, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3918, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4515, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4158, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4069, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3862, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3694, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4079, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3902, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4093, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4045, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3966, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3875, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4038, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4133, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3973, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3992, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4293, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.387, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3934, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3799, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3831, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3882, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3894, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3818, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3872, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3953, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3959, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3974, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4108, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4145, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3752, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4069, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3727, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3951, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3986, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3822, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3899, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3907, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3837, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.398, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4259, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3786, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3825, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4114, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3871, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3931, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3705, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3838, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3864, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3895, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3818, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3721, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3784, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3884, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3891, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3886, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4102, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3778, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3958, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3805, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4049, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3776, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3828, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3796, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.397, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3607, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3724, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3849, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4087, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3927, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3893, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.406, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3842, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3956, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3639, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4054, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.402, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3798, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3806, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4029, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3859, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3813, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3829, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3961, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3897, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3975, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3765, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3864, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3913, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3987, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.357, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3797, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3771, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4064, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.389, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3677, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3456, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3914, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4165, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4322, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3381, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3991, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3859, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3811, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3647, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.392, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3781, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3884, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3853, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3849, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3682, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3752, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3584, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3817, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.383, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3786, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3865, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3782, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.41, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3618, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3819, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3756, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4114, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3673, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3846, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3573, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3593, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3705, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3566, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4043, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3637, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.403, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3707, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3883, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3855, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3787, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.386, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3948, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.396, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3726, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3786, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3967, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.378, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3762, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4001, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.404, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3638, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3735, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3977, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3894, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3809, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3908, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3755, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3632, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3792, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3629, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3898, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3723, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3646, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4049, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3695, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3745, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3862, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.365, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.356, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3843, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3651, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3542, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3948, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3742, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3812, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3642, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3722, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3565, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3708, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3585, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.363, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3586, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3603, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3395, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3794, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3892, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3993, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3686, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3984, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4079, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3834, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.411, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3583, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3532, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3825, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3567, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3718, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3608, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3835, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.4081, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3541, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3505, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3748, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3767, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3689, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.377, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3716, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3393, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3885, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3473, 'learning_rate': 2e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3847, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3695, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3553, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3689, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3967, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3891, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3757, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.366, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3744, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3649, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3641, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3657, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3458, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3696, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3812, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3694, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.406, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3732, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4011, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3923, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3688, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3944, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3602, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3506, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3677, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3668, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.357, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3972, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.351, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3704, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3637, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3731, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3902, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.398, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.384, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4071, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3605, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3638, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3815, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3595, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3868, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3764, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3699, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3676, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3693, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3764, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3761, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3785, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3909, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3675, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3852, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3645, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3691, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3557, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3728, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3561, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3556, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3488, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3482, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3769, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3691, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3841, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3744, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3798, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.372, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3499, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4035, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3496, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3649, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.383, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3564, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3922, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3697, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3479, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3444, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3465, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3533, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3857, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3712, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3569, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3603, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3467, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3534, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.355, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3436, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3514, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3655, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3654, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3827, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3382, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3457, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3587, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3831, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3887, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3657, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3734, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3735, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3359, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3642, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3537, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.366, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3604, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3727, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3547, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3427, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.347, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3895, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3601, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3641, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3372, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4003, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3611, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3441, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3398, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3477, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.382, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3571, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3978, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3203, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3596, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4154, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3376, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3622, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3685, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3476, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3354, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3666, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3796, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3806, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3529, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3699, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3744, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3525, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3692, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3608, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3736, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3426, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3487, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3746, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.356, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3366, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3536, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3547, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.345, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3535, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.355, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3593, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3329, 'learning_rate': 2e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3822, 'learning_rate': 2e-05, 'epoch': 0.02}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/phi-2-mlb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=\"../models/phi-2-mlb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# base model\n",
    "base_path=\"../models/phi-2\"  \n",
    "\n",
    "# adapters: path to folder with adapter_model.safetensors\n",
    "adapter_path=\"../models/phi-2-mlb/checkpoint-448\" \n",
    "\n",
    "# where to save merged model\n",
    "save_to=\"../models/phi-2-mlb/\"       \n",
    "\n",
    "# Load model and tokenizer\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_path)\n",
    "\n",
    "# Add/set tokens same tokens to base model before merging, like we did before training  \n",
    "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
    "\n",
    "base_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set a default Generation configuration: Llama precise\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=100, \n",
    "    temperature=0.7,\n",
    "    top_p=0.1,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.18,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Load LoRA and merge\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.save_pretrained(save_to, safe_serialization=True, max_shard_size='4GB')\n",
    "tokenizer.save_pretrained(save_to)\n",
    "generation_config.save_pretrained(save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_path=\"../models/phi-2-mlb/\"   \n",
    "\n",
    "prompt=\"Instruct: {\\\"input\\\": {\\\"pitcher\\\": {\\\"id\\\": 460024, \\\"name\\\": \\\"luke hochevar\\\"}, \\\"batter\\\": {\\\"id\\\": 110029, \\\"name\\\": \\\"bobby abreu\\\"}, \\\"p_throws\\\": \\\"R\\\", \\\"stand\\\": \\\"L\\\", \\\"inning_topbot\\\": \\\"Top\\\", \\\"inning\\\": 1, \\\"outs_when_up\\\": 1, \\\"on_1b\\\": \\\"\\\", \\\"on_2b\\\": {\\\"id\\\": 435062, \\\"name\\\": \\\"howie kendrick\\\"}, \\\"on_3b\\\": \\\"\\\", \\\"home_score\\\": 0, \\\"away_score\\\": 0}}? \\n\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,    \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path) \n",
    "\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output_tokens = model.generate(**input_tokens)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "    output_tokens[0][len(input_tokens[0]):],\n",
    "    skip_special_tokens=True\n",
    "    )               \n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
