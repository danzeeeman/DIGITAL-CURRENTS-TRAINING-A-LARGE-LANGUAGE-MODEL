{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tHOTZYrmevi"
      },
      "outputs": [],
      "source": [
        "%pip install einops\n",
        "%pip install peft\n",
        "%pip install trl\n",
        "%pip install tensorboard\n",
        "%pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl\n",
        "%pip install tokenizers\n",
        "%pip install torch==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install torchaudio==2.1.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install torchvision==0.16.2+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install ipywidgets\n",
        "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "%pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "%pip install -q -U datasets scipy ipywidgets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CHECK TO MAKE SURE YOU GOT A GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z0cRlXC-zLX",
        "outputId": "91ab35ee-91db-4683-a738-bb5c2c89c365"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper Functions to normalize your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R-M63LcMAJEr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_special_characters_and_spaces(input_string):\n",
        "    # Define a regular expression pattern to match special characters and spaces\n",
        "    pattern = r'[^a-zA-Z0-9]+'  # This pattern will keep only letters and digits\n",
        "\n",
        "    # Use the sub method to replace matches of the pattern with an empty string\n",
        "    clean_string = re.sub(pattern, '', input_string)\n",
        "\n",
        "    return clean_string\n",
        "\n",
        "def remove_special_characters(input_string):\n",
        "    # Define a regular expression pattern to match special characters and spaces\n",
        "    pattern = r'[^a-zA-Z0-9\\s]+'  # This pattern will keep only letters and digits\n",
        "\n",
        "    # Use the sub method to replace matches of the pattern with an empty string\n",
        "    clean_string = re.sub(pattern, '', input_string)\n",
        "\n",
        "    return clean_string"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Proto Prompting "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converting Your CSV to JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_csv_to_json(csv_file_path):\n",
        "    # Read CSV file\n",
        "    with open(csv_file_path, 'r') as file:\n",
        "        reader = csv.DictReader(file)\n",
        "        rows = list(reader)\n",
        "    # Convert CSV data to JSON\n",
        "    json_data = json.dumps(rows, indent=4)\n",
        "\n",
        "    # Save JSON data to a file (optional)\n",
        "    with open(f'{csv_file_path}.json', 'w') as json_file:\n",
        "        json_file.write(json_data)\n",
        "\n",
        "    return json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cjGs9KzDmKHl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion completed. JSON data:\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "csv_file_path = '../data/All Playlists Combined.csv'\n",
        "\n",
        "# Convert CSV to JSON\n",
        "json_data = convert_csv_to_json(csv_file_path)\n",
        "\n",
        "print(\"Conversion completed. JSON data:\")\n",
        "# print(json_data)\n",
        "json_data = json.loads(json_data)\n",
        "songs = {}\n",
        "songs['train']={\n",
        "    'target':[],\n",
        "    'lyrics':[],\n",
        "    'id':[]\n",
        "}\n",
        "songs['validation']={\n",
        "    'target':[],\n",
        "    'lyrics':[],\n",
        "    'id':[]\n",
        "}\n",
        "songs['test']={\n",
        "    'target':[],\n",
        "    'lyrics':[],\n",
        "    'id':[]\n",
        "}\n",
        "count = 1\n",
        "for i in json_data:\n",
        "    track_name = remove_special_characters(i[\"track_name\"])\n",
        "    lyrics = remove_special_characters(i[\"lyrics\"])\n",
        "    if random.randint(0, 125125124) < 125125124/100:\n",
        "        songs['test']['id'].append(f\"test-{count}\")\n",
        "        songs['test']['target'].append(track_name)\n",
        "        songs['test']['lyrics'].append(lyrics)\n",
        "        songs['validation']['id'].append(f\"validation-{count}\")\n",
        "        songs['validation']['target'].append(track_name)\n",
        "        songs['validation']['lyrics'].append(lyrics)\n",
        "    else:\n",
        "        songs['train']['id'].append(f\"train-{count}\")\n",
        "        songs['train']['target'].append(track_name)\n",
        "        songs['train']['lyrics'].append(lyrics)\n",
        "    count+=1\n",
        "with open(f'../data/songs.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(songs, f, ensure_ascii=True, indent=4)\n",
        "    f.close()          "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine-tuning a Mixtral-7b "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "train_dataset = load_dataset(\"json\", data_files='../data/songs.json',field=\"train\", split='all')\n",
        "eval_dataset = load_dataset(\"json\", data_files='../data/songs.json', field=\"validation\", split='all') \n",
        "test_dataset = load_dataset(\"json\", data_files='../data/songs.json', field=\"test\", split='all')\n",
        "\n",
        "dataset = DatasetDict(\n",
        "    {\n",
        "        \"train\":train_dataset,\n",
        "        \"validation\":eval_dataset,\n",
        "        \"test\":test_dataset\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['target', 'lyrics', 'id'],\n",
              "        num_rows: 1241\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['target', 'lyrics', 'id'],\n",
              "        num_rows: 1241\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['target', 'lyrics', 'id'],\n",
              "        num_rows: 1241\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "base_model_id = \"../models/Mistral-7B-Instruct-v0.2\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    model_max_length=512,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(prompt):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt =f\"\"\"Given a target song title, write a christmas song that corresponds to the target song title.\n",
        "\n",
        "\n",
        "### Target song title:\n",
        "{data_point[\"target\"]}\n",
        "\n",
        "\n",
        "### Song Lyrics:\n",
        "{data_point[\"lyrics\"]}\n",
        "\"\"\"\n",
        "    return tokenize(full_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbec60168fc244a195379f86e22655ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1241 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 12628, 264, 2718, 4034, 3941, 28725, 3324, 264, 22735, 4876, 4034, 369, 16535, 298, 272, 2718, 4034, 3941, 28723, 13, 13, 13, 27332, 15255, 4034, 3941, 28747, 13, 12341, 3604, 944, 330, 5625, 643, 9999, 7416, 13, 13, 13, 27332, 11509, 393, 19591, 28747, 13, 13, 13, 28792, 5760, 331, 28705, 28740, 28793, 13, 12341, 3936, 264, 3051, 643, 1628, 7416, 13, 8779, 574, 3031, 347, 2061, 13, 6693, 879, 544, 813, 21477, 13, 12695, 347, 575, 302, 7739, 13, 13, 28792, 5760, 331, 28705, 28750, 28793, 13, 12341, 3936, 264, 3051, 643, 1628, 7416, 13, 13806, 272, 26424, 895, 547, 11997, 13, 6693, 879, 544, 813, 21477, 13, 12695, 347, 6052, 1753, 13, 13, 28792, 5760, 331, 28705, 28770, 28793, 13, 16114, 1076, 390, 297, 1571, 269, 2202, 13, 28769, 9813, 13863, 2202, 302, 337, 431, 13, 28765, 28708, 372, 1007, 3282, 693, 460, 13095, 298, 592, 13, 12695, 347, 3065, 298, 592, 2327, 680, 13, 13, 28792, 2919, 311, 28793, 13, 28735, 16812, 339, 3403, 478, 544, 622, 347, 2553, 13, 3381, 272, 285, 1002, 1914, 13, 25072, 868, 478, 28742, 584, 506, 298, 290, 12370, 291, 1059, 9864, 13, 5142, 506, 3936, 264, 3051, 643, 1628, 7416, 1055, 13, 13, 13, 2]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_train_dataset[4]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target song title: It's Beginning to Look a Lot Like Christmas\n",
            "Song Lyrics: \n",
            "\n",
            "It's beginning to look a lot like Christmas\n",
            "Everywhere you go\n",
            "Take a look in the five-and-ten\n",
            "Glistening once again\n",
            "With candy canes and silver lanes aglow\n",
            "\n",
            "It's beginning to look a lot like Christmas\n",
            "Toys in every store\n",
            "But the prettiest sight to see\n",
            "Is the holly that will be\n",
            "On your own front door\n",
            "\n",
            "A pair of hop along boots and a pistol that shoots\n",
            "Is the wish of Barney and Ben\n",
            "Dolls that will talk and will go for a walk\n",
            "Is the hope of Janice and Jen\n",
            "And mom and dad can hardly wait for school to start again\n",
            "\n",
            "It's beginning to look a lot like Christmas\n",
            "Everywhere you go\n",
            "Now there's a tree in the Grand Hotel\n",
            "One in the park as well\n",
            "The sturdy kind that doesn't mind the snow\n",
            "\n",
            "It's beginning to look a lot like Christmas\n",
            "Soon the bells will start\n",
            "And the thing that will make them ring\n",
            "Is the carol that you sing\n",
            "Right within your heart\n",
            "\n",
            "A pair of hop along boots and a pistol that shoots\n",
            "Is the wish of Barney and Ben\n",
            "Dolls that will talk and will go for a walk\n",
            "Is the hope of Janice and Jen\n",
            "And mom and dad can hardly wait for school to start again\n",
            "\n",
            "Ha Ha Ha Ha Ha Ha Ha Ha Ha Ha Ha Ha Ha Ha!\n",
            "\n",
            "It's beginning to look a lot like Christmas\n",
            "Soon the bells will start\n",
            "And the thing that will make them ring\n",
            "Is the carol that you sing\n",
            "Right within your heart\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Target song title: \" + test_dataset[1]['target'])\n",
        "print(\"Song Lyrics: \" + test_dataset[1]['lyrics'] + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_prompt = f\"\"\"Given a target song title, provide the lyrics of song that corresponds to the target song title.\n",
        "\n",
        "\n",
        "### Target song title:\n",
        "    {test_dataset[1]['target']}\n",
        "\n",
        "\n",
        "### Song Lyrics:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given a target song title, provide the lyrics of song that corresponds to the target song title.\n",
            "\n",
            "\n",
            "### Target song title:\n",
            "    It's Beginning to Look a Lot Like Christmas\n",
            "\n",
            "\n",
            "### Song Lyrics:\n",
            " 1.\n",
            "(Verse 1)\n",
            "What's that coming over the hill,\n",
            "Is it a man with a gun,\n",
            "Or a sleepy old town,\n",
            "Or just a few more days 'til Christmas?\n",
            "\n",
            "It's beginning to look a lot like Christmas,\n",
            "Everywhere I go.\n",
            "Stores are filled with bright and shiny things,\n",
            "And families gather together in joy.\n",
            "\n",
            "2.\n",
            "(Verse 2)\n",
            "Snow on the ground,\n",
            "Gifts wrapped and placed under the tree for all to see,\n",
            "Candy canes on shelves in every store,\n",
            "And children listening,\n",
            "Listening for sleigh bells in the snow.\n",
            "\n",
            "It's beginning to look a lot like Christmas,\n",
            "Everywhere I go.\n",
            "There'll be much to do,\n",
            "With just a week 'til Christmas day.\n",
            "\n",
            "3.\n",
            "(Verse 3)\n",
            "There'll be parties for hosting,\n",
            "Marshmallows for toasting,\n",
            "And caroling out in the snow.\n",
            "There'll be noises,\n",
            "And bright lights,\n",
            "Telling me,\n",
            "Yes, it's beginning to look a lot like Christmas\n"
          ]
        }
      ],
      "source": [
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=256, padding_side='left',pad_token_id=2)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)\n",
        "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
        "model = accelerator.prepare_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): MistralForCausalLM(\n",
            "      (model): MistralModel(\n",
            "        (embed_tokens): Embedding(32000, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x MistralDecoderLayer(\n",
            "            (self_attn): MistralSdpaAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (rotary_emb): MistralRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): MistralMLP(\n",
            "              (gate_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): MistralRMSNorm()\n",
            "            (post_attention_layernorm): MistralRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): MistralRMSNorm()\n",
            "      )\n",
            "      (lm_head): lora.Linear(\n",
            "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "project = \"xmas-finetune\"\n",
        "base_model_name = \"mistral\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"../models/\" + run_name\n",
        "\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=5,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        max_steps=5000,\n",
        "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
        "        logging_steps=1,\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=100,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=500,\n",
        "        save_total_limit=3,              # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training          # Comment this out if you don't want to use weights & baises\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint=\"../models/mistral-xmas-finetune/checkpoint-900\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\src\\transformer-sketchbook\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2b7eb3b439747838bab022c4d269695",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
          ]
        }
      ],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "adapter_path = \"../models/mistral-xmas-finetune/checkpoint-900\"\n",
        "ft_model = PeftModel.from_pretrained(base_model, adapter_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_prompt =\"\"\"Given a target song title, write a christmas song that corresponds to the target song title.\n",
        "\n",
        "\n",
        "### Target song title:\n",
        "Twas once a moonlit night\n",
        "\n",
        "\n",
        "### Song Lyrics:\n",
        "\"\"\"\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ft_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given a target song title, write a christmas song that corresponds to the target song title.\n",
            "\n",
            "\n",
            "### Target song title:\n",
            "Twas once a moonlit night\n",
            "\n",
            "\n",
            "### Song Lyrics:\n",
            "\n",
            "\n",
            "[Verse 1]\n",
            "Oh by gosh by golly gee\n",
            "It's the big fat Christmas family\n",
            "I wanna sing about the greatest thing\n",
            "That ever happened on the 25th thing\n",
            "So let's all join together friends\n",
            "And sing about the greatest thing that ever was\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "\n",
            "[Chorus]\n",
            "Joy to the world it's a moonlit night\n",
            "Joy to the world it's a moonlit night\n",
            "Born in a stable so cold\n",
            "Blessed is the world with the holy birth\n",
            "Of the Savior of the world\n",
            "Born on a cold December night\n",
            "\n",
            "[Verse 2]\n",
            "Lay your head on that old cabin door\n",
            "Come and listen to the old men four\n",
            "Sing about the greatest thing that\n",
            "The Lord was born in Bethlehem\n",
            "So you will know the day will come\n",
            "When he'll come again and take you home\n",
            "So by gosh by golly wow\n",
            "He'll come again and take you home\n",
            "\n",
            "[Chorus]\n",
            "Joy to the world it's a moonlit night\n",
            "Joy to the world it's a moonlit night\n",
            "Born in a stable so cold\n",
            "Blessed is the world with the holy birth\n",
            "Of the Savior of the world\n",
            "Born on a cold December night\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "On a cold December night\n",
            "\n",
            "[Verse 3]\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "\n",
            "\n",
            "### Song Lyrics in English:\n",
            "\n",
            "\n",
            "[Verse 1]\n",
            "Oh by gosh by golly gee\n",
            "It's the big fat Christmas family\n",
            "I wanna sing about the greatest thing\n",
            "That ever happened on the 25th thing\n",
            "So let's all join together friends\n",
            "And sing about the greatest thing that ever was\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "\n",
            "[Chorus]\n",
            "Joy to the world it's a moonlit night\n",
            "Joy to the world it's a moonlit night\n",
            "Born in a stable so cold\n",
            "Blessed is the world with the holy birth\n",
            "Of the Savior of the world\n",
            "Born on a cold December night\n",
            "\n",
            "[Verse 2]\n",
            "Lay your head on that old cabin door\n",
            "Come and listen to the old men four\n",
            "Sing about the greatest thing that\n",
            "The Lord was born in Bethlehem\n",
            "So you will know the day will come\n",
            "When he'll come again and take you home\n",
            "So by gosh by golly wow\n",
            "He'll come again and take you home\n",
            "\n",
            "[Chorus]\n",
            "Joy to the world it's a moonlit night\n",
            "Joy to the world it's a moonlit night\n",
            "Born in a stable so cold\n",
            "Blessed is the world with the holy birth\n",
            "Of the Savior of the world\n",
            "Born on a cold December night\n",
            "\n",
            "[Verse 3]\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "Born on a cold December night\n",
            "The greatest thing was born\n",
            "\n",
            "\n",
            "### Notes:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=1024, pad_token_id=2)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_to=\"../models/mixtral-xmas-finetune\" \n",
        "ft_model.save_pretrained(save_to, safe_serialization=True, max_shard_size='4GB')\n",
        "tokenizer.save_pretrained(save_to)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
