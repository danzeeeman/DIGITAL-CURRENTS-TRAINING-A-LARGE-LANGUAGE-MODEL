{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to generate text: using different decoding methods for language generation with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of '../models/mlb_structured/checkpoint-148500'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../models/mlb_structured/checkpoint-148500' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    629\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    630\u001b[0m         configuration_file,\n\u001b[0;32m    631\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    632\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    633\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    634\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    635\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    636\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    637\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    638\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    639\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    640\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    641\u001b[0m     )\n\u001b[0;32m    642\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:112\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 112\u001b[0m     validate_repo_id(arg_value)\n\u001b[0;32m    114\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../models/mlb_structured/checkpoint-148500'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32me:\\src\\transformer-sketchbook\\notebooks\\generating_structured.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch_device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# To use a different branch, change revision\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# For example: revision=\"main\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                              device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                              trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                              revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mto(torch_device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path, use_fast\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/generating_structured.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m prompt \u001b[39m=\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpitcher\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshaun marcum\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatter\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mjeff keppinger\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mp_throws\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstand\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39minning_topbot\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBot\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39minning\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: 1, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mouts_when_up\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: 0, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon_1b\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon_2b\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mandres torres\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mon_3b\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhome_score\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: 0, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39maway_score\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: 0}, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'''\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:441\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m kwargs_copy\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    439\u001b[0m         _ \u001b[39m=\u001b[39m kwargs_copy\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 441\u001b[0m     config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    442\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    443\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    444\u001b[0m         trust_remote_code\u001b[39m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    445\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    446\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_copy,\n\u001b[0;32m    447\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n\u001b[0;32m    449\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:916\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    915\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 916\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    918\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    572\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 573\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    575\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\configuration_utils.py:649\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    648\u001b[0m         \u001b[39m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[1;32m--> 649\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    650\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load the configuration of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    651\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m name. Otherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    653\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    654\u001b[0m         )\n\u001b[0;32m    656\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    657\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load the configuration of '../models/mlb_structured/checkpoint-148500'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../models/mlb_structured/checkpoint-148500' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name_or_path = \"../models/mlb_structured/checkpoint-148500\"\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             revision=\"main\").to(torch_device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "\n",
    "prompt ='''{\"input\": {\"pitcher\": \"shaun marcum\", \"batter\": \"jeff keppinger\", \"p_throws\": \"R\", \"stand\": \"R\", \"inning_topbot\": \"Bot\", \"inning\": 1, \"outs_when_up\": 0, \"on_1b\": \"\", \"on_2b\": \"andres torres\", \"on_3b\": \"\", \"home_score\": 0, \"away_score\": 0}, \"result\":'''\n",
    "model_inputs = tokenizer(prompt, return_tensors='pt').to(torch_device)\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{\" input \": {\" pitcher \": \" shaun marcum \", \" batter \": \" jeff keppinger \", \" p_throws \": \" R \", \" stand \": \" R \", \" inning_topbot \": \" Bot \", \" inning \": 1, \" outs_when_up \": 0, \" on_1b \": \"\", \" on_2b \": \" andres torres \", \" on_3b \": \"\", \" home_score \": 0, \" away_score \": 0 }, \" result \": {\" event \": \" walk \", \" type \": \" B \", \" zone \": 14, \" des \": \" Jeff Keppinger walks. Andres Torres scores. Andres Torres to 2nd.\", \" at_bat_number \": 5, \" pitch_number \": 5, \" pitch_name \": \" Slider \", \" hit_location \": \"\", \" launch_speed \": 1, \" at_bat_number \": 5, \" brett myers \", \" \", \" shaun marcum \", \" brett myers fontenot \", \" shaun marcum \", \" at_bat_number \": 5, \" Slider \", \" at_bat_number \": 5, \" Changeup \", \" Jackson \", \" Slider \", \" henry sosa \", \" brett myers \": 0, \" brett myers \": 5, \" at_bat_number \": \" home_score \": \" inning \": \" brett myers \": \" at_bat_number \": \" at_bat_number \": 5, \" p_throws \": \" p_throws \": \" p_throws \": \" Slider \", \" at_bat_number \": \" henry sosa away_score \": \" jackson \": \" at_bat_number \": \" carlos zambrano pence \", \" at_bat_number \": \" miguel tejada \", \" p_throws \": \" pitch_number \": 5, \" G. walks, \" Slider \", \" interference by pitch \": \" Slider \", \" p_throws \": 5, \" Slider \", \" p_throws \": \" p_throws \": \" brett myers \": \" henry sosa \": \" interference by pitch \": \" at_bat_number \": \" brett myers \": \" home_score \": \" brett myers \": \" Slider \", \" p_throws \": \" manny machado \", \" at_bat_number \": \" fielder Ryan Hanigan \", \" Slider \", \" p_throws \": \" p_throws \": \" p_throws \": \" Slider \", \" henry sosa \": \" pitch_number \": \" pitch \": \" p_throws \": \" hit_location \": \" on_3b \": \" on_3b \": \" carlos corporan \", \" steve pearce \": \" home_score \": \" Slider \", \" on_3b \": \" brett myers \": \" brett myers \": \" home_score \": \" hit_location \": \" manny \" jackson \": \" brett myers \", \" Slider \", \" p_throws \": \" Jose Altuve, \" Brett Myers \": \" p_throws \": 5, \" p_throws \": \" Slider \", \" fielder Ryan Hanigan \", \" Slider \", \" p_throws \": \" Slider \", \" p_throws \": \" Slider \", \" hit_location \": \" p_throws \": \" p_throws \": \" Slider \", \" Chris Johnson R \", \" brett myers \": \" Brett Myers \": \" Slider \", \" hit_location \": \" carlos corporan \": \" on_3b \": \" hit_location \": \" Slider \", \" p_throws \": \" p_throws \": \" Slider \", \" on_3b \": \" Slider \", \" hit_location \": \" p_throws \": \" Slider \", \" henry sosa \": \" hit_location \": \" p_throws \": \" Slider \", \" hit_location \": \" p_throws \": \" hit_location \": \" Slider \", \" home_score \": \" p_throws \": \" Slider \", \" steve pearce \", \" p_throws \": \" hit_location \": \" Slider \", \" Slider \", \" p_throws \": \" Slider \", \" Slider \",\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=512)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "### instruction : what is the outcome of pitcher 663432 pitching to batter 596115 ### input : Top of the 1 inning with 2 outs ### output : 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for a ball, 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=65,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "### instruction : what is the outcome of pitcher 663432 pitching to batter 596115 ### input : Top of the 8 inning with 2 outs ### output : 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for for strike, 663432 throws a 88 miles per hour Cutter for a ball, 663432 throws a 88 miles per hour Slider and 596115 hits into a field out, event : field out, event : field out, out, event : field out\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=65,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=10,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: ### instruction : what is the outcome of pitcher 663432 pitching to batter 596115 ### input : Top of the 8 inning with 2 outs ### output : 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for for strike, 663432 throws a 88 miles per hour Cutter for a ball, 663432 throws a 88 miles per hour Slider and 596115 hits into a field out, event : field out, event : field out, out, event : field out\n",
      "1: ### instruction : what is the outcome of pitcher 663432 pitching to batter 596115 ### input : Top of the 8 inning with 2 outs ### output : 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for for strike, 663432 throws a 88 miles per hour Changeup for a ball, 663432 throws a 88 miles per hour Slider and 596115 hits into a field out, event : field out, event : field out, out, event : field out\n",
      "2: ### instruction : what is the outcome of pitcher 663432 pitching to batter 596115 ### input : Top of the 8 inning with 2 outs ### output : 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for stealing strike, 663432 throws a 88 miles per hour Cutter for a ball, 663432 throws a 88 miles per hour Slider and 596115 hits into a field out, event : field out, event : field out, out, event : field out\n",
      "3: ### instruction : what is the outcome of pitcher 663432 pitching to batter 596115 ### input : Top of the 8 inning with 2 outs ### output : 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for for strike, 663432 throws a 88 miles per hour Cutter for a ball, 663432 throws a 88 miles per hour Slider and 596115 hits into a field out, event : field out, event : field out, out, field out, event\n",
      "4: ### instruction : what is the outcome of pitcher 663432 pitching to batter 596115 ### input : Top of the 8 inning with 2 outs ### output : 663432 throws a 88 miles per hour Slider for a strike, 663432 throws a 88 miles per hour Slider for for strike, 663432 throws a 88 miles per hour Cutter for a ball, 663432 throws a 88 miles per hour Slider and 596115 hits into a field out, event : field out, event : field out, out, hits : field out\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=65,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=10,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-p (nucleus) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a custom decoding strategy with your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"my_account/my_model\")\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n",
    ")\n",
    "generation_config.save_pretrained(\"my_account/my_model\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
