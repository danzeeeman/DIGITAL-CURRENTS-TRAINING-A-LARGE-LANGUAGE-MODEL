{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Everything You Need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tokenizers\n",
        "%pip install transformers\n",
        "%pip install datasets --upgrade"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gI1Tp54IiVBj"
      },
      "source": [
        "## Train a custom tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "712\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from pathlib import Path\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[    \n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\"\n",
        "    ])\n",
        "\n",
        "tokenizer.train(files=[\"twenty_years_of_baseball_structed_data.txt\"], trainer=trainer)\n",
        "tokenizer.save(\"../models/mlb_structured/tokenizer.json\")\n",
        "\n",
        "print(len('{\"input\": {\"pitcher\": \"jered weaver\", \"batter\": \"alcides escobar\", \"p_throws\": \"R\", \"stand\": \"R\", \"inning_topbot\": \"Bot\", \"inning\": 5, \"outs_when_up\": 1, \"on_1b\": \"\", \"on_2b\": \"\", \"on_3b\": \"\", \"home_score\": 0, \"away_score\": 2}, \"result\": {\"event\": \"field_out\", \"type\": \"X\", \"zone\": 14, \"des\": \"Alcides Escobar grounds out softly, third baseman Maicer Izturis to first baseman Mark Trumbo.\", \"at_bat_number\": 40, \"pitch_number\": 5, \"pitch_name\": \"Slider\", \"hit_location\": 5, \"launch_speed\": \"\", \"launch_speed_angle\": \"\", \"runs_scored\": 0, \"at_bat\": [\"called_strike\", \"called_strike\", \"foul\", \"ball\", \"hit_into_play\"], \"pitch_type\": [\"FF\", \"FF\", \"SI\", \"SI\", \"SL\"], \"release_speed\": [88.3, 90.2, 90.2, 88.6, 80.5]}}'))\n",
        "# output = tokenizer.encode()\n",
        "# print(output.tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11491"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.get_vocab_size()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=11491,\n",
        "    max_position_embeddings=1024,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast(tokenizer_file=\"../models/mlb_structured/tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaForCausalLM, RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForCausalLM(config=config)\n",
        "# model = RobertaForMaskedLM(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52744675"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./twenty_years_of_baseball_structed_data.txt\",\n",
        "    block_size=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    tf32=True,\n",
        "    output_dir=\"../models/mlb_structured\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=500,\n",
        "    per_device_train_batch_size=512,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "506b9db195324f72a1b971b6425ac5d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/15570000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.3084, 'learning_rate': 4.999839434810533e-05, 'epoch': 0.32}\n",
            "{'loss': 0.5488, 'learning_rate': 4.999678869621066e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4676, 'learning_rate': 4.9995183044316e-05, 'epoch': 0.96}\n",
            "{'loss': 0.3985, 'learning_rate': 4.9993577392421324e-05, 'epoch': 1.28}\n",
            "{'loss': 0.2833, 'learning_rate': 4.999197174052666e-05, 'epoch': 1.61}\n",
            "{'loss': 0.2373, 'learning_rate': 4.9990366088631985e-05, 'epoch': 1.93}\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a86659043ef84fc78db4d7eb9b79ff4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/778500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1108, 'learning_rate': 4.826589595375723e-05, 'epoch': 17.34}\n",
            "{'loss': 0.1107, 'learning_rate': 4.8233782915863844e-05, 'epoch': 17.66}\n",
            "{'loss': 0.1109, 'learning_rate': 4.820166987797046e-05, 'epoch': 17.98}\n",
            "{'loss': 0.1101, 'learning_rate': 4.816955684007707e-05, 'epoch': 18.3}\n",
            "{'loss': 0.1103, 'learning_rate': 4.813744380218369e-05, 'epoch': 18.63}\n",
            "{'loss': 0.1096, 'learning_rate': 4.81053307642903e-05, 'epoch': 18.95}\n",
            "{'loss': 0.1092, 'learning_rate': 4.807321772639692e-05, 'epoch': 19.27}\n",
            "{'loss': 0.1089, 'learning_rate': 4.8041104688503536e-05, 'epoch': 19.59}\n",
            "{'loss': 0.1088, 'learning_rate': 4.800899165061015e-05, 'epoch': 19.91}\n",
            "{'loss': 0.1091, 'learning_rate': 4.7976878612716764e-05, 'epoch': 20.23}\n",
            "{'loss': 0.1085, 'learning_rate': 4.794476557482338e-05, 'epoch': 20.55}\n",
            "{'loss': 0.1082, 'learning_rate': 4.791265253693e-05, 'epoch': 20.87}\n",
            "{'loss': 0.1079, 'learning_rate': 4.7880539499036607e-05, 'epoch': 21.19}\n",
            "{'loss': 0.1077, 'learning_rate': 4.784842646114323e-05, 'epoch': 21.52}\n",
            "{'loss': 0.1072, 'learning_rate': 4.781631342324984e-05, 'epoch': 21.84}\n",
            "{'loss': 0.1077, 'learning_rate': 4.7784200385356456e-05, 'epoch': 22.16}\n",
            "{'loss': 0.1074, 'learning_rate': 4.775208734746307e-05, 'epoch': 22.48}\n",
            "{'loss': 0.1075, 'learning_rate': 4.7719974309569684e-05, 'epoch': 22.8}\n",
            "{'loss': 0.1066, 'learning_rate': 4.7687861271676305e-05, 'epoch': 23.12}\n",
            "{'loss': 0.1063, 'learning_rate': 4.765574823378292e-05, 'epoch': 23.44}\n",
            "{'loss': 0.1065, 'learning_rate': 4.7623635195889534e-05, 'epoch': 23.76}\n",
            "{'loss': 0.1062, 'learning_rate': 4.759152215799615e-05, 'epoch': 24.08}\n",
            "{'loss': 0.1063, 'learning_rate': 4.755940912010277e-05, 'epoch': 24.41}\n",
            "{'loss': 0.1058, 'learning_rate': 4.7527296082209376e-05, 'epoch': 24.73}\n",
            "{'loss': 0.1061, 'learning_rate': 4.7495183044316e-05, 'epoch': 25.05}\n",
            "{'loss': 0.1055, 'learning_rate': 4.746307000642261e-05, 'epoch': 25.37}\n",
            "{'loss': 0.106, 'learning_rate': 4.7430956968529225e-05, 'epoch': 25.69}\n",
            "{'loss': 0.1058, 'learning_rate': 4.739884393063584e-05, 'epoch': 26.01}\n",
            "{'loss': 0.1054, 'learning_rate': 4.7366730892742454e-05, 'epoch': 26.33}\n",
            "{'loss': 0.1053, 'learning_rate': 4.7334617854849075e-05, 'epoch': 26.65}\n",
            "{'loss': 0.1056, 'learning_rate': 4.730250481695568e-05, 'epoch': 26.97}\n",
            "{'loss': 0.1051, 'learning_rate': 4.72703917790623e-05, 'epoch': 27.3}\n",
            "{'loss': 0.1051, 'learning_rate': 4.723827874116892e-05, 'epoch': 27.62}\n",
            "{'loss': 0.1049, 'learning_rate': 4.720616570327553e-05, 'epoch': 27.94}\n",
            "{'loss': 0.1048, 'learning_rate': 4.7174052665382146e-05, 'epoch': 28.26}\n",
            "{'loss': 0.1046, 'learning_rate': 4.714193962748876e-05, 'epoch': 28.58}\n",
            "{'loss': 0.1045, 'learning_rate': 4.710982658959538e-05, 'epoch': 28.9}\n",
            "{'loss': 0.1044, 'learning_rate': 4.707771355170199e-05, 'epoch': 29.22}\n",
            "{'loss': 0.1041, 'learning_rate': 4.704560051380861e-05, 'epoch': 29.54}\n",
            "{'loss': 0.1038, 'learning_rate': 4.7013487475915223e-05, 'epoch': 29.87}\n",
            "{'loss': 0.1039, 'learning_rate': 4.6981374438021844e-05, 'epoch': 30.19}\n",
            "{'loss': 0.1034, 'learning_rate': 4.694926140012845e-05, 'epoch': 30.51}\n",
            "{'loss': 0.1038, 'learning_rate': 4.6917148362235066e-05, 'epoch': 30.83}\n",
            "{'loss': 0.103, 'learning_rate': 4.688503532434169e-05, 'epoch': 31.15}\n",
            "{'loss': 0.1029, 'learning_rate': 4.6852922286448294e-05, 'epoch': 31.47}\n",
            "{'loss': 0.1032, 'learning_rate': 4.6820809248554915e-05, 'epoch': 31.79}\n",
            "{'loss': 0.1029, 'learning_rate': 4.678869621066153e-05, 'epoch': 32.11}\n",
            "{'loss': 0.1028, 'learning_rate': 4.675658317276815e-05, 'epoch': 32.43}\n",
            "{'loss': 0.1025, 'learning_rate': 4.672447013487476e-05, 'epoch': 32.76}\n",
            "{'loss': 0.103, 'learning_rate': 4.669235709698138e-05, 'epoch': 33.08}\n",
            "{'loss': 0.1029, 'learning_rate': 4.666024405908799e-05, 'epoch': 33.4}\n",
            "{'loss': 0.1024, 'learning_rate': 4.662813102119461e-05, 'epoch': 33.72}\n",
            "{'loss': 0.1022, 'learning_rate': 4.659601798330122e-05, 'epoch': 34.04}\n",
            "{'loss': 0.1024, 'learning_rate': 4.6563904945407836e-05, 'epoch': 34.36}\n",
            "{'loss': 0.1025, 'learning_rate': 4.653179190751446e-05, 'epoch': 34.68}\n",
            "{'loss': 0.1023, 'learning_rate': 4.6499678869621064e-05, 'epoch': 35.0}\n",
            "{'loss': 0.1024, 'learning_rate': 4.6467565831727685e-05, 'epoch': 35.32}\n",
            "{'loss': 0.1022, 'learning_rate': 4.64354527938343e-05, 'epoch': 35.65}\n",
            "{'loss': 0.1017, 'learning_rate': 4.640333975594091e-05, 'epoch': 35.97}\n",
            "{'loss': 0.1016, 'learning_rate': 4.637122671804753e-05, 'epoch': 36.29}\n",
            "{'loss': 0.1018, 'learning_rate': 4.633911368015414e-05, 'epoch': 36.61}\n",
            "{'loss': 0.1018, 'learning_rate': 4.630700064226076e-05, 'epoch': 36.93}\n",
            "{'loss': 0.1013, 'learning_rate': 4.627488760436737e-05, 'epoch': 37.25}\n",
            "{'loss': 0.1018, 'learning_rate': 4.624277456647399e-05, 'epoch': 37.57}\n",
            "{'loss': 0.1015, 'learning_rate': 4.6210661528580605e-05, 'epoch': 37.89}\n",
            "{'loss': 0.101, 'learning_rate': 4.6178548490687226e-05, 'epoch': 38.21}\n",
            "{'loss': 0.1012, 'learning_rate': 4.6146435452793834e-05, 'epoch': 38.54}\n",
            "{'loss': 0.1009, 'learning_rate': 4.6114322414900455e-05, 'epoch': 38.86}\n",
            "{'loss': 0.1013, 'learning_rate': 4.608220937700707e-05, 'epoch': 39.18}\n",
            "{'loss': 0.1008, 'learning_rate': 4.605009633911368e-05, 'epoch': 39.5}\n",
            "{'loss': 0.1011, 'learning_rate': 4.60179833012203e-05, 'epoch': 39.82}\n",
            "{'loss': 0.1009, 'learning_rate': 4.598587026332691e-05, 'epoch': 40.14}\n",
            "{'loss': 0.1002, 'learning_rate': 4.595375722543353e-05, 'epoch': 40.46}\n",
            "{'loss': 0.1006, 'learning_rate': 4.592164418754014e-05, 'epoch': 40.78}\n",
            "{'loss': 0.1005, 'learning_rate': 4.588953114964676e-05, 'epoch': 41.1}\n",
            "{'loss': 0.1009, 'learning_rate': 4.5857418111753375e-05, 'epoch': 41.43}\n",
            "{'loss': 0.1007, 'learning_rate': 4.582530507385999e-05, 'epoch': 41.75}\n",
            "{'loss': 0.1003, 'learning_rate': 4.57931920359666e-05, 'epoch': 42.07}\n",
            "{'loss': 0.1001, 'learning_rate': 4.576107899807322e-05, 'epoch': 42.39}\n",
            "{'loss': 0.0998, 'learning_rate': 4.572896596017984e-05, 'epoch': 42.71}\n",
            "{'loss': 0.0999, 'learning_rate': 4.5696852922286446e-05, 'epoch': 43.03}\n",
            "{'loss': 0.1007, 'learning_rate': 4.566473988439307e-05, 'epoch': 43.35}\n",
            "{'loss': 0.0996, 'learning_rate': 4.563262684649968e-05, 'epoch': 43.67}\n",
            "{'loss': 0.1004, 'learning_rate': 4.56005138086063e-05, 'epoch': 43.99}\n",
            "{'loss': 0.0995, 'learning_rate': 4.556840077071291e-05, 'epoch': 44.32}\n",
            "{'loss': 0.1005, 'learning_rate': 4.553628773281953e-05, 'epoch': 44.64}\n",
            "{'loss': 0.0996, 'learning_rate': 4.5504174694926145e-05, 'epoch': 44.96}\n",
            "{'loss': 0.0998, 'learning_rate': 4.547206165703275e-05, 'epoch': 45.28}\n",
            "{'loss': 0.0998, 'learning_rate': 4.543994861913937e-05, 'epoch': 45.6}\n",
            "{'loss': 0.0995, 'learning_rate': 4.540783558124599e-05, 'epoch': 45.92}\n",
            "{'loss': 0.0999, 'learning_rate': 4.537572254335261e-05, 'epoch': 46.24}\n",
            "{'loss': 0.0992, 'learning_rate': 4.5343609505459215e-05, 'epoch': 46.56}\n",
            "{'loss': 0.0991, 'learning_rate': 4.5311496467565836e-05, 'epoch': 46.89}\n",
            "{'loss': 0.0991, 'learning_rate': 4.527938342967245e-05, 'epoch': 47.21}\n",
            "{'loss': 0.0994, 'learning_rate': 4.5247270391779065e-05, 'epoch': 47.53}\n",
            "{'loss': 0.0988, 'learning_rate': 4.521515735388568e-05, 'epoch': 47.85}\n",
            "{'loss': 0.0992, 'learning_rate': 4.518304431599229e-05, 'epoch': 48.17}\n",
            "{'loss': 0.0989, 'learning_rate': 4.5150931278098914e-05, 'epoch': 48.49}\n",
            "{'loss': 0.099, 'learning_rate': 4.511881824020552e-05, 'epoch': 48.81}\n",
            "{'loss': 0.0992, 'learning_rate': 4.508670520231214e-05, 'epoch': 49.13}\n",
            "{'loss': 0.0985, 'learning_rate': 4.505459216441876e-05, 'epoch': 49.45}\n",
            "{'loss': 0.099, 'learning_rate': 4.502247912652537e-05, 'epoch': 49.78}\n",
            "{'loss': 0.0988, 'learning_rate': 4.4990366088631985e-05, 'epoch': 50.1}\n",
            "{'loss': 0.0987, 'learning_rate': 4.49582530507386e-05, 'epoch': 50.42}\n",
            "{'loss': 0.0991, 'learning_rate': 4.492614001284522e-05, 'epoch': 50.74}\n",
            "{'loss': 0.0986, 'learning_rate': 4.489402697495183e-05, 'epoch': 51.06}\n",
            "{'loss': 0.0985, 'learning_rate': 4.486191393705845e-05, 'epoch': 51.38}\n",
            "{'loss': 0.0983, 'learning_rate': 4.482980089916506e-05, 'epoch': 51.7}\n",
            "{'loss': 0.0982, 'learning_rate': 4.4797687861271684e-05, 'epoch': 52.02}\n",
            "{'loss': 0.0981, 'learning_rate': 4.476557482337829e-05, 'epoch': 52.34}\n",
            "{'loss': 0.0982, 'learning_rate': 4.473346178548491e-05, 'epoch': 52.67}\n",
            "{'loss': 0.0987, 'learning_rate': 4.4701348747591526e-05, 'epoch': 52.99}\n",
            "{'loss': 0.0978, 'learning_rate': 4.466923570969814e-05, 'epoch': 53.31}\n",
            "{'loss': 0.0979, 'learning_rate': 4.4637122671804755e-05, 'epoch': 53.63}\n",
            "{'loss': 0.0984, 'learning_rate': 4.460500963391137e-05, 'epoch': 53.95}\n",
            "{'loss': 0.0974, 'learning_rate': 4.457289659601799e-05, 'epoch': 54.27}\n",
            "{'loss': 0.0979, 'learning_rate': 4.45407835581246e-05, 'epoch': 54.59}\n",
            "{'loss': 0.0979, 'learning_rate': 4.450867052023122e-05, 'epoch': 54.91}\n",
            "{'loss': 0.0977, 'learning_rate': 4.447655748233783e-05, 'epoch': 55.23}\n",
            "{'loss': 0.098, 'learning_rate': 4.4444444444444447e-05, 'epoch': 55.56}\n",
            "{'loss': 0.0976, 'learning_rate': 4.441233140655106e-05, 'epoch': 55.88}\n",
            "{'loss': 0.0975, 'learning_rate': 4.4380218368657675e-05, 'epoch': 56.2}\n",
            "{'loss': 0.0976, 'learning_rate': 4.4348105330764296e-05, 'epoch': 56.52}\n",
            "{'loss': 0.0979, 'learning_rate': 4.43159922928709e-05, 'epoch': 56.84}\n",
            "{'loss': 0.0971, 'learning_rate': 4.4283879254977524e-05, 'epoch': 57.16}\n",
            "{'loss': 0.0975, 'learning_rate': 4.425176621708414e-05, 'epoch': 57.48}\n",
            "{'loss': 0.0977, 'learning_rate': 4.421965317919075e-05, 'epoch': 57.8}\n",
            "{'loss': 0.0977, 'learning_rate': 4.418754014129737e-05, 'epoch': 58.12}\n",
            "{'loss': 0.0969, 'learning_rate': 4.415542710340399e-05, 'epoch': 58.45}\n",
            "{'loss': 0.0976, 'learning_rate': 4.41233140655106e-05, 'epoch': 58.77}\n",
            "{'loss': 0.0969, 'learning_rate': 4.409120102761721e-05, 'epoch': 59.09}\n",
            "{'loss': 0.0974, 'learning_rate': 4.405908798972383e-05, 'epoch': 59.41}\n",
            "{'loss': 0.0968, 'learning_rate': 4.4026974951830445e-05, 'epoch': 59.73}\n",
            "{'loss': 0.0974, 'learning_rate': 4.3994861913937066e-05, 'epoch': 60.05}\n",
            "{'loss': 0.0967, 'learning_rate': 4.396274887604367e-05, 'epoch': 60.37}\n",
            "{'loss': 0.0969, 'learning_rate': 4.3930635838150294e-05, 'epoch': 60.69}\n",
            "{'loss': 0.0975, 'learning_rate': 4.389852280025691e-05, 'epoch': 61.01}\n",
            "{'loss': 0.0968, 'learning_rate': 4.386640976236352e-05, 'epoch': 61.34}\n",
            "{'loss': 0.0965, 'learning_rate': 4.3834296724470136e-05, 'epoch': 61.66}\n",
            "{'loss': 0.0965, 'learning_rate': 4.380218368657675e-05, 'epoch': 61.98}\n",
            "{'loss': 0.0964, 'learning_rate': 4.377007064868337e-05, 'epoch': 62.3}\n",
            "{'loss': 0.0965, 'learning_rate': 4.373795761078998e-05, 'epoch': 62.62}\n",
            "{'loss': 0.0969, 'learning_rate': 4.37058445728966e-05, 'epoch': 62.94}\n",
            "{'loss': 0.0963, 'learning_rate': 4.3673731535003214e-05, 'epoch': 63.26}\n",
            "{'loss': 0.0964, 'learning_rate': 4.364161849710983e-05, 'epoch': 63.58}\n",
            "{'loss': 0.0964, 'learning_rate': 4.360950545921644e-05, 'epoch': 63.9}\n",
            "{'loss': 0.096, 'learning_rate': 4.357739242132306e-05, 'epoch': 64.23}\n",
            "{'loss': 0.0962, 'learning_rate': 4.354527938342968e-05, 'epoch': 64.55}\n",
            "{'loss': 0.0965, 'learning_rate': 4.3513166345536285e-05, 'epoch': 64.87}\n",
            "{'loss': 0.0965, 'learning_rate': 4.3481053307642906e-05, 'epoch': 65.19}\n",
            "{'loss': 0.096, 'learning_rate': 4.344894026974952e-05, 'epoch': 65.51}\n",
            "{'loss': 0.0961, 'learning_rate': 4.3416827231856134e-05, 'epoch': 65.83}\n",
            "{'loss': 0.0956, 'learning_rate': 4.338471419396275e-05, 'epoch': 66.15}\n",
            "{'loss': 0.0961, 'learning_rate': 4.335260115606937e-05, 'epoch': 66.47}\n",
            "{'loss': 0.0959, 'learning_rate': 4.3320488118175984e-05, 'epoch': 66.8}\n",
            "{'loss': 0.0963, 'learning_rate': 4.32883750802826e-05, 'epoch': 67.12}\n",
            "{'loss': 0.096, 'learning_rate': 4.325626204238921e-05, 'epoch': 67.44}\n",
            "{'loss': 0.0962, 'learning_rate': 4.3224149004495826e-05, 'epoch': 67.76}\n",
            "{'loss': 0.0957, 'learning_rate': 4.319203596660244e-05, 'epoch': 68.08}\n",
            "{'loss': 0.0956, 'learning_rate': 4.3159922928709055e-05, 'epoch': 68.4}\n",
            "{'loss': 0.0957, 'learning_rate': 4.3127809890815676e-05, 'epoch': 68.72}\n",
            "{'loss': 0.0959, 'learning_rate': 4.309569685292229e-05, 'epoch': 69.04}\n",
            "{'loss': 0.0959, 'learning_rate': 4.3063583815028904e-05, 'epoch': 69.36}\n",
            "{'loss': 0.0956, 'learning_rate': 4.303147077713552e-05, 'epoch': 69.69}\n",
            "{'loss': 0.0957, 'learning_rate': 4.299935773924213e-05, 'epoch': 70.01}\n",
            "{'loss': 0.0958, 'learning_rate': 4.2967244701348753e-05, 'epoch': 70.33}\n",
            "{'loss': 0.0952, 'learning_rate': 4.293513166345536e-05, 'epoch': 70.65}\n",
            "{'loss': 0.0955, 'learning_rate': 4.290301862556198e-05, 'epoch': 70.97}\n",
            "{'loss': 0.0952, 'learning_rate': 4.2870905587668596e-05, 'epoch': 71.29}\n",
            "{'loss': 0.0954, 'learning_rate': 4.283879254977521e-05, 'epoch': 71.61}\n",
            "{'loss': 0.0948, 'learning_rate': 4.2806679511881824e-05, 'epoch': 71.93}\n",
            "{'loss': 0.0954, 'learning_rate': 4.2774566473988445e-05, 'epoch': 72.25}\n",
            "{'loss': 0.0956, 'learning_rate': 4.274245343609506e-05, 'epoch': 72.58}\n",
            "{'loss': 0.0953, 'learning_rate': 4.2710340398201674e-05, 'epoch': 72.9}\n",
            "{'loss': 0.0948, 'learning_rate': 4.267822736030829e-05, 'epoch': 73.22}\n",
            "{'loss': 0.095, 'learning_rate': 4.26461143224149e-05, 'epoch': 73.54}\n",
            "{'loss': 0.0951, 'learning_rate': 4.2614001284521516e-05, 'epoch': 73.86}\n",
            "{'loss': 0.0953, 'learning_rate': 4.258188824662813e-05, 'epoch': 74.18}\n",
            "{'loss': 0.0948, 'learning_rate': 4.254977520873475e-05, 'epoch': 74.5}\n",
            "{'loss': 0.0949, 'learning_rate': 4.2517662170841366e-05, 'epoch': 74.82}\n",
            "{'loss': 0.0949, 'learning_rate': 4.248554913294798e-05, 'epoch': 75.14}\n",
            "{'loss': 0.0945, 'learning_rate': 4.2453436095054594e-05, 'epoch': 75.47}\n",
            "{'loss': 0.0945, 'learning_rate': 4.242132305716121e-05, 'epoch': 75.79}\n",
            "{'loss': 0.0947, 'learning_rate': 4.238921001926782e-05, 'epoch': 76.11}\n",
            "{'loss': 0.0943, 'learning_rate': 4.2357096981374437e-05, 'epoch': 76.43}\n",
            "{'loss': 0.0943, 'learning_rate': 4.232498394348106e-05, 'epoch': 76.75}\n",
            "{'loss': 0.094, 'learning_rate': 4.229287090558767e-05, 'epoch': 77.07}\n",
            "{'loss': 0.0942, 'learning_rate': 4.2260757867694286e-05, 'epoch': 77.39}\n",
            "{'loss': 0.0939, 'learning_rate': 4.22286448298009e-05, 'epoch': 77.71}\n",
            "{'loss': 0.0943, 'learning_rate': 4.2196531791907514e-05, 'epoch': 78.03}\n",
            "{'loss': 0.0943, 'learning_rate': 4.2164418754014135e-05, 'epoch': 78.36}\n",
            "{'loss': 0.0939, 'learning_rate': 4.213230571612074e-05, 'epoch': 78.68}\n",
            "{'loss': 0.0945, 'learning_rate': 4.2100192678227364e-05, 'epoch': 79.0}\n",
            "{'loss': 0.0942, 'learning_rate': 4.206807964033398e-05, 'epoch': 79.32}\n",
            "{'loss': 0.0937, 'learning_rate': 4.203596660244059e-05, 'epoch': 79.64}\n",
            "{'loss': 0.0941, 'learning_rate': 4.2003853564547206e-05, 'epoch': 79.96}\n",
            "{'loss': 0.0941, 'learning_rate': 4.197174052665383e-05, 'epoch': 80.28}\n",
            "{'loss': 0.0941, 'learning_rate': 4.193962748876044e-05, 'epoch': 80.6}\n",
            "{'loss': 0.0939, 'learning_rate': 4.1907514450867055e-05, 'epoch': 80.92}\n",
            "{'loss': 0.0937, 'learning_rate': 4.187540141297367e-05, 'epoch': 81.25}\n",
            "{'loss': 0.0934, 'learning_rate': 4.1843288375080284e-05, 'epoch': 81.57}\n",
            "{'loss': 0.0937, 'learning_rate': 4.18111753371869e-05, 'epoch': 81.89}\n",
            "{'loss': 0.0935, 'learning_rate': 4.177906229929351e-05, 'epoch': 82.21}\n",
            "{'loss': 0.0932, 'learning_rate': 4.174694926140013e-05, 'epoch': 82.53}\n",
            "{'loss': 0.0941, 'learning_rate': 4.171483622350675e-05, 'epoch': 82.85}\n",
            "{'loss': 0.0937, 'learning_rate': 4.168272318561336e-05, 'epoch': 83.17}\n",
            "{'loss': 0.0934, 'learning_rate': 4.1650610147719976e-05, 'epoch': 83.49}\n",
            "{'loss': 0.0934, 'learning_rate': 4.161849710982659e-05, 'epoch': 83.82}\n",
            "{'loss': 0.0935, 'learning_rate': 4.1586384071933204e-05, 'epoch': 84.14}\n",
            "{'loss': 0.0935, 'learning_rate': 4.155427103403982e-05, 'epoch': 84.46}\n",
            "{'loss': 0.0934, 'learning_rate': 4.152215799614644e-05, 'epoch': 84.78}\n",
            "{'loss': 0.0931, 'learning_rate': 4.1490044958253053e-05, 'epoch': 85.1}\n",
            "{'loss': 0.0934, 'learning_rate': 4.145793192035967e-05, 'epoch': 85.42}\n",
            "{'loss': 0.093, 'learning_rate': 4.142581888246628e-05, 'epoch': 85.74}\n",
            "{'loss': 0.0935, 'learning_rate': 4.13937058445729e-05, 'epoch': 86.06}\n",
            "{'loss': 0.093, 'learning_rate': 4.136159280667952e-05, 'epoch': 86.38}\n",
            "{'loss': 0.0928, 'learning_rate': 4.132947976878613e-05, 'epoch': 86.71}\n",
            "{'loss': 0.0928, 'learning_rate': 4.1297366730892745e-05, 'epoch': 87.03}\n",
            "{'loss': 0.0929, 'learning_rate': 4.126525369299936e-05, 'epoch': 87.35}\n",
            "{'loss': 0.0925, 'learning_rate': 4.1233140655105974e-05, 'epoch': 87.67}\n",
            "{'loss': 0.093, 'learning_rate': 4.120102761721259e-05, 'epoch': 87.99}\n",
            "{'loss': 0.0927, 'learning_rate': 4.116891457931921e-05, 'epoch': 88.31}\n",
            "{'loss': 0.0928, 'learning_rate': 4.113680154142582e-05, 'epoch': 88.63}\n",
            "{'loss': 0.0927, 'learning_rate': 4.110468850353244e-05, 'epoch': 88.95}\n",
            "{'loss': 0.0929, 'learning_rate': 4.107257546563905e-05, 'epoch': 89.27}\n",
            "{'loss': 0.0924, 'learning_rate': 4.1040462427745666e-05, 'epoch': 89.6}\n",
            "{'loss': 0.0926, 'learning_rate': 4.100834938985228e-05, 'epoch': 89.92}\n",
            "{'loss': 0.0924, 'learning_rate': 4.0976236351958894e-05, 'epoch': 90.24}\n",
            "{'loss': 0.0921, 'learning_rate': 4.0944123314065515e-05, 'epoch': 90.56}\n",
            "{'loss': 0.0924, 'learning_rate': 4.091201027617213e-05, 'epoch': 90.88}\n",
            "{'loss': 0.0924, 'learning_rate': 4.087989723827874e-05, 'epoch': 91.2}\n",
            "{'loss': 0.0924, 'learning_rate': 4.084778420038536e-05, 'epoch': 91.52}\n",
            "{'loss': 0.0921, 'learning_rate': 4.081567116249198e-05, 'epoch': 91.84}\n",
            "{'loss': 0.0921, 'learning_rate': 4.0783558124598586e-05, 'epoch': 92.16}\n",
            "{'loss': 0.0922, 'learning_rate': 4.07514450867052e-05, 'epoch': 92.49}\n",
            "{'loss': 0.0925, 'learning_rate': 4.071933204881182e-05, 'epoch': 92.81}\n",
            "{'loss': 0.0923, 'learning_rate': 4.0687219010918435e-05, 'epoch': 93.13}\n",
            "{'loss': 0.0922, 'learning_rate': 4.065510597302505e-05, 'epoch': 93.45}\n",
            "{'loss': 0.092, 'learning_rate': 4.0622992935131664e-05, 'epoch': 93.77}\n",
            "{'loss': 0.0912, 'learning_rate': 4.0590879897238285e-05, 'epoch': 94.09}\n",
            "{'loss': 0.0914, 'learning_rate': 4.055876685934489e-05, 'epoch': 94.41}\n",
            "{'loss': 0.0916, 'learning_rate': 4.052665382145151e-05, 'epoch': 94.73}\n",
            "{'loss': 0.0921, 'learning_rate': 4.049454078355813e-05, 'epoch': 95.05}\n",
            "{'loss': 0.0919, 'learning_rate': 4.046242774566474e-05, 'epoch': 95.38}\n",
            "{'loss': 0.0917, 'learning_rate': 4.0430314707771356e-05, 'epoch': 95.7}\n",
            "{'loss': 0.0918, 'learning_rate': 4.039820166987797e-05, 'epoch': 96.02}\n",
            "{'loss': 0.0914, 'learning_rate': 4.036608863198459e-05, 'epoch': 96.34}\n",
            "{'loss': 0.0918, 'learning_rate': 4.0333975594091205e-05, 'epoch': 96.66}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32me:\\src\\transformer-sketchbook\\notebooks\\training_an_slm.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/src/transformer-sketchbook/notebooks/training_an_slm.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../models/mlb_structured/checkpoint-26500\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2715\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[0;32m   2716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2717\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2719\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\danm\\Downloads\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train(resume_from_checkpoint=\"../models/mlb_structured/checkpoint-26500\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"./models/mlb/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPqbqeA0VB70ho26cHWVp02",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "smallBERTa_Pretraining.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
