{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Everything You Need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install tokenizers\n",
        "%pip install transformers\n",
        "%pip install datasets --upgrade"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gI1Tp54IiVBj"
      },
      "source": [
        "## Train a custom tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['what', 'is', 'the', 'outcome', 'of', 'pitcher', '461833', 'pitching', 'to', 'batter', '435079']\n"
          ]
        }
      ],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from pathlib import Path\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[    \n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\"\n",
        "    ])\n",
        "\n",
        "tokenizer.train(files=[\"./twenty_years_of_baseball_2.txt\"], trainer=trainer)\n",
        "tokenizer.save(\"./models/mlb_short/tokenizer/tokenizer.json\")\n",
        "\n",
        "output = tokenizer.encode(\"what is the outcome of pitcher 461833 pitching to batter 435079\")\n",
        "print(output.tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11481"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.get_vocab_size()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train a Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=11481,\n",
        "    max_position_embeddings=514,\n",
        "    num_attention_heads=12,\n",
        "    num_hidden_layers=6,\n",
        "    type_vocab_size=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast(tokenizer_file=\"./models/mlb/tokenizer/baseball.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bin c:\\Users\\NVIDIA\\Downloads\\text-generation-webui-1.6\\text-generation-webui-1.6\\installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaForCausalLM, RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForCausalLM(config=config)\n",
        "# model = RobertaForMaskedLM(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52345305"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data_file = {\"train\":\"./baseball_23_03.txt\"}\n",
        "# dataset = load_dataset(\"json\", data_files=data_file)\n",
        "dataset = load_dataset(\"text\", data_files=data_file)\n",
        "# raw_datasets = load_dataset(\n",
        "#             \"json\",\n",
        "#             data_files=data_file,\n",
        "#         )\n",
        "\n",
        "# raw_datasets = {}\n",
        "# raw_datasets[\"validation\"] = load_dataset(\n",
        "#     \"json\",\n",
        "#     data_files=data_file,\n",
        "#     split=f\"train[:{90}%]\",\n",
        "# )\n",
        "# raw_datasets[\"train\"] = load_dataset(\n",
        "#     \"json\",\n",
        "#     data_files=data_file,\n",
        "#     split=f\"train[:{10}%]\",\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3678821\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset[\"train\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\NVIDIA\\Downloads\\text-generation-webui-1.6\\text-generation-webui-1.6\\installer_files\\env\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "\n",
        "dataset = LineByLineTextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"./baseball_23_03.txt\",\n",
        "    block_size=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    tf32=True,\n",
        "    output_dir=\"./models/mlb\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=100,\n",
        "    per_device_train_batch_size=512,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint=\"./models/mlb/checkpoint-503500\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"./models/mlb/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"./models/mlb/checkpoint-498000\",\n",
        "    tokenizer=tokenizer,\n",
        "    top_k=20,\n",
        ")\n",
        "\n",
        "fill_text = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"./models/mlb/checkpoint-498000\",\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "https://huggingface.co/blog/how-to-generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"./models/mlb/checkpoint-498000\", pad_token_id=tokenizer.eos_token_id).to(torch_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encode context the generation is conditioned on\n",
        "model_inputs = tokenizer(f'<s> ###instruction: what is the outcome of pitcher 663432 pitching to batter 596115 ###input: Top of the {math.floor(random.randrange(1, 10))} inning with {math.floor(random.randrange(0, 3))} outs ###output: 663432 throws a\"', return_tensors='pt').to(torch_device)\n",
        "\n",
        "# generate 40 new tokens\n",
        "greedy_output = model.generate(**model_inputs, max_new_tokens=160)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=5,\n",
        "    num_return_sequences=5,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# now we have 3 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "for i in range(0, 15):\n",
        "    model_inputs = tokenizer(f'<s> ###instruction: what is the outcome of pitcher 663432 pitching to batter 596115 ###input: Top of the {math.floor(random.randrange(1, 10))} inning with {math.floor(random.randrange(0, 3))} outs ###output: 663432 throws a\"', return_tensors='pt').to(torch_device)\n",
        "\n",
        "    # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "    sample_output = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=40,\n",
        "        do_sample=True,\n",
        "        top_k=100,\n",
        "        temperature=0.6,\n",
        "    )\n",
        "\n",
        "    print(\"Output:\\n\" + 100 * '-')\n",
        "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=0,\n",
        "    temperature=0.6,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "set_seed(42)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "set_seed(42)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(0, 50):\n",
        "    text = fill_text(f\"<s> ###instruction: what is the outcome of pitcher 663432 pitching to batter 596115 ###input: Top of the {math.floor(random.randrange(1, 10))} inning with {math.floor(random.randrange(0, 3))} outs ###output: 663432 throws a\", max_new_tokens=25, top_k=10, temperature=1.25, do_sample=True, epsilon_cutoff=9e-4, encoder_repetition_penalty=0.1)\n",
        "    for t in text:\n",
        "        print(t)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPqbqeA0VB70ho26cHWVp02",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "smallBERTa_Pretraining.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
